{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "from IPython.display import display\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def plot_accuracy(train_acc, val_acc):\n",
    "    plt.title('FOOD11-ResNet50')\n",
    "    plt.plot(train_acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n",
    "    plt.show()\n",
    "def plot_loss(train_loss, val_loss):\n",
    "    plt.title('FOOD11-ResNet50')\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'validation_loss'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-1.8268, -1.9980, -1.9980,  ...,  1.2043,  1.2214,  1.2214],\n",
      "         [-1.6898, -1.8782, -1.9980,  ...,  1.2385,  1.2385,  1.2385],\n",
      "         [-1.7069, -1.7925, -1.9467,  ...,  1.2214,  1.2385,  1.2557],\n",
      "         ...,\n",
      "         [ 1.4612,  1.4612,  1.4783,  ...,  0.8961,  0.9303,  0.9646],\n",
      "         [ 1.4440,  1.3413,  1.2557,  ...,  0.8789,  0.9132,  0.9474],\n",
      "         [ 1.2043,  1.1015,  1.0331,  ...,  0.8789,  0.9132,  0.9303]],\n",
      "\n",
      "        [[-1.1078, -1.3880, -1.4755,  ...,  1.2731,  1.2906,  1.3081],\n",
      "         [-0.9853, -1.2654, -1.4580,  ...,  1.2731,  1.2731,  1.2906],\n",
      "         [-1.0028, -1.1604, -1.3529,  ...,  1.2556,  1.2556,  1.2731],\n",
      "         ...,\n",
      "         [ 1.6057,  1.5882,  1.6057,  ...,  0.6078,  0.6779,  0.7654],\n",
      "         [ 1.5182,  1.3782,  1.2731,  ...,  0.5378,  0.6254,  0.6604],\n",
      "         [ 1.1506,  1.0630,  0.9405,  ...,  0.4678,  0.5553,  0.5903]],\n",
      "\n",
      "        [[-1.2293, -1.4210, -1.4733,  ...,  1.8731,  1.8731,  1.9428],\n",
      "         [-1.1247, -1.3164, -1.4907,  ...,  1.8731,  1.8557,  1.9080],\n",
      "         [-1.2119, -1.3164, -1.4559,  ...,  1.8557,  1.8208,  1.8731],\n",
      "         ...,\n",
      "         [ 1.9080,  1.9254,  1.9254,  ...,  1.0714,  1.1585,  1.2631],\n",
      "         [ 1.7337,  1.6117,  1.4897,  ...,  1.0017,  1.0888,  1.1585],\n",
      "         [ 1.2457,  1.1237,  0.9842,  ...,  0.9319,  1.0191,  1.0714]]]), 10)\n",
      "11966\n",
      "(tensor([[[-0.7650, -0.7650, -0.7822,  ..., -0.7308, -0.7308, -0.7308],\n",
      "         [-0.7650, -0.7822, -0.7822,  ..., -0.7308, -0.7308, -0.7308],\n",
      "         [-0.7822, -0.7822, -0.7822,  ..., -0.7308, -0.7308, -0.7308],\n",
      "         ...,\n",
      "         [ 1.4098,  1.4440,  1.4440,  ...,  1.7865,  1.7694,  1.7694],\n",
      "         [ 1.3413,  1.3755,  1.3927,  ...,  1.7523,  1.7523,  1.7523],\n",
      "         [ 1.3242,  1.2899,  1.2728,  ...,  1.7523,  1.7694,  1.7694]],\n",
      "\n",
      "        [[-1.1954, -1.1954, -1.2129,  ..., -1.4405, -1.4405, -1.4405],\n",
      "         [-1.1954, -1.2129, -1.2129,  ..., -1.4405, -1.4405, -1.4405],\n",
      "         [-1.2129, -1.2129, -1.2304,  ..., -1.4405, -1.4405, -1.4405],\n",
      "         ...,\n",
      "         [ 0.6779,  0.7479,  0.7654,  ...,  1.9559,  1.9734,  1.9734],\n",
      "         [ 0.7304,  0.7479,  0.7654,  ...,  1.9559,  1.9559,  1.9559],\n",
      "         [ 0.7304,  0.6954,  0.6779,  ...,  1.9559,  1.9559,  1.9559]],\n",
      "\n",
      "        [[-0.8633, -0.8633, -0.8807,  ..., -1.3513, -1.3513, -1.3513],\n",
      "         [-0.8633, -0.8807, -0.8807,  ..., -1.3513, -1.3513, -1.3513],\n",
      "         [-0.8807, -0.8807, -0.8807,  ..., -1.3513, -1.3513, -1.3513],\n",
      "         ...,\n",
      "         [ 1.0365,  1.0888,  1.1237,  ...,  1.9603,  1.9603,  1.9603],\n",
      "         [ 1.0888,  1.1062,  1.1585,  ...,  1.9428,  1.9254,  1.9254],\n",
      "         [ 1.1062,  1.0714,  1.0714,  ...,  1.9254,  1.9080,  1.8905]]]), 4)\n",
      "1330\n",
      "(tensor([[[ 1.7009,  1.7009,  1.7352,  ...,  1.4269,  1.4612,  1.4269],\n",
      "         [ 1.8037,  1.7694,  1.7865,  ...,  1.5639,  1.5468,  1.5125],\n",
      "         [ 1.8722,  1.8208,  1.8379,  ...,  1.7180,  1.6838,  1.6153],\n",
      "         ...,\n",
      "         [ 0.3309,  0.1426, -0.2171,  ..., -0.3712, -0.2171, -0.3712],\n",
      "         [ 0.3823,  0.2796, -0.0116,  ..., -0.3541, -0.3027, -0.4568],\n",
      "         [ 0.4337,  0.3823,  0.2111,  ..., -0.4226, -0.4911, -0.4739]],\n",
      "\n",
      "        [[ 1.1856,  1.2031,  1.2206,  ..., -1.1779, -1.3179, -1.4230],\n",
      "         [ 1.2381,  1.2031,  1.2031,  ..., -0.9153, -1.1604, -1.3354],\n",
      "         [ 1.1856,  1.1506,  1.1681,  ..., -0.6001, -0.9328, -1.1604],\n",
      "         ...,\n",
      "         [-1.7381, -1.7731, -1.8957,  ..., -0.1275,  0.0301, -0.1275],\n",
      "         [-1.7206, -1.7206, -1.8256,  ..., -0.1625, -0.0924, -0.2500],\n",
      "         [-1.7031, -1.7031, -1.7381,  ..., -0.2500, -0.3375, -0.3200]],\n",
      "\n",
      "        [[ 0.6879,  0.6879,  0.7054,  ..., -0.9330, -1.0724, -1.1770],\n",
      "         [ 0.6705,  0.6705,  0.6705,  ..., -0.6890, -0.9156, -1.0898],\n",
      "         [ 0.5485,  0.5136,  0.5485,  ..., -0.4275, -0.7064, -0.9156],\n",
      "         ...,\n",
      "         [-1.5081, -1.5430, -1.6476,  ..., -1.1596, -1.0898, -1.3164],\n",
      "         [-1.5604, -1.5256, -1.5953,  ..., -1.1770, -1.2119, -1.4210],\n",
      "         [-1.5604, -1.5430, -1.5430,  ..., -1.2293, -1.4036, -1.4559]]]), 0)\n",
      "3347\n"
     ]
    }
   ],
   "source": [
    "# diretory of data\n",
    "img_dir = [r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\\training\", \n",
    "           r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\\validation\"]\n",
    "length_data_in_trainingSet = len(glob.glob(img_dir[0]+\"\\*\"))\n",
    "\n",
    "\n",
    "class trainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, train_list):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.train_list = train_list\n",
    "        self.img_names = [self.root_dir + os.sep + item for item in self.train_list]\n",
    "        self.labels = [int(item.split(os.sep)[1].split('_')[0]) for item in self.train_list]\n",
    "\n",
    "        self.transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                             #transforms.RandomRotation(180),\n",
    "                                             transforms.ColorJitter(brightness=(0.3 if random.random()<0.5 else False),\n",
    "                                                                    contrast=(0.2 if random.random()<0.5 else False),\n",
    "                                                                    saturation=(0.2 if random.random()<0.5 else False),\n",
    "                                                                    hue=(0.1 if random.random()<0.5 else False)),\n",
    "                                             #transforms.RandomAffine(10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=(0.15, 0), resample=False),\n",
    "                                             # transforms.RandomRotation(45),\n",
    "                                             # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "                                             # transforms.Resize((448, 448)),\n",
    "                                             \n",
    "                                              transforms.Resize((256, 256)),\n",
    "                                              transforms.RandomCrop((224, 224)),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                   [0.229, 0.224, 0.225])\n",
    "                                            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.img_names[i]).convert('RGB')\n",
    "        return self.transform(img), self.labels[i]\n",
    "\n",
    "\n",
    "\n",
    "class valDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, val_list):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.val_list = val_list\n",
    "        self.img_names = [self.root_dir + os.sep + item for item in self.val_list]\n",
    "        self.labels = [int(item.split(os.sep)[1].split('_')[0]) for item in self.val_list]\n",
    "\n",
    "        # PyTorch transforms\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                             transforms.CenterCrop((224, 224)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                  [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.img_names[i]).convert('RGB')\n",
    "        return self.transform(img), self.labels[i]\n",
    "\n",
    "\n",
    "def random_split_train_val(train_dir, val_dir, split_ratio):\n",
    "    train_names = ['training' + os.sep + item for item in os.listdir(train_dir)]\n",
    "    val_names = ['validation' + os.sep + item for item in os.listdir(val_dir)]\n",
    "    all_names = train_names + val_names\n",
    "    nums = len(all_names)\n",
    "    ratio = split_ratio\n",
    "    random.shuffle(all_names)\n",
    "    new_val = all_names[:round(nums * ratio)]\n",
    "    new_train = all_names[round(nums * ratio):]\n",
    "    return new_train, new_val\n",
    "\n",
    "root_dir = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\food-11\"\n",
    "train_dir = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\\training\\\\\"\n",
    "val_dir = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\\validation\\\\\"\n",
    "new_train, new_val = random_split_train_val(train_dir, val_dir, 0.1)\n",
    "train_set = trainDataset(r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\", new_train)\n",
    "print(train_set[0])\n",
    "print(len(train_set))\n",
    "val_set = valDataset(r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\", new_val)\n",
    "print(val_set[0])\n",
    "print(len(val_set))\n",
    "\n",
    "val2_names = os.listdir(root_dir + os.sep + \"evaluation\" + os.sep)\n",
    "val2_list = [\"evaluation\" + os.sep + item for item in val2_names]\n",
    "val2_set = valDataset(root_dir, val2_list)\n",
    "\n",
    "print(val2_set[0])\n",
    "print(len(val2_set))\n",
    "# ind = 7000\n",
    "# train_data = trainDataset(img_dir)\n",
    "# print(len(train_data))\n",
    "# val_data = valDataset([img_dir[1]])\n",
    "# img, label = train_data[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenEfficientNet(\n",
      "  (conv_stem): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): DepthwiseSeparableConv(\n",
      "        (conv_dw): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(20, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(20, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)\n",
      "          (1): Conv2d(80, 80, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=80, bias=False)\n",
      "          (2): Conv2d(80, 80, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=80, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(120, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(120, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (2): Conv2d(72, 72, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=72, bias=False)\n",
      "          (3): Conv2d(72, 72, kernel_size=(9, 9), stride=(2, 2), padding=(4, 4), groups=72, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(288, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(24, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(32, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(32, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(32, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(32, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "          (1): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)\n",
      "          (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=128, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "          (2): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "          (3): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "          (2): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "          (3): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "          (2): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "          (3): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "          (2): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "          (3): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(64, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (2): Conv2d(144, 144, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=144, bias=False)\n",
      "          (3): Conv2d(144, 144, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=144, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (2): Conv2d(144, 144, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=144, bias=False)\n",
      "          (3): Conv2d(144, 144, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=144, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (2): Conv2d(144, 144, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=144, bias=False)\n",
      "          (3): Conv2d(144, 144, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=144, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv_pw): MixedConv2d(\n",
      "          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (2): Conv2d(144, 144, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=144, bias=False)\n",
      "          (3): Conv2d(144, 144, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=144, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
      "          (1): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (2): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)\n",
      "          (3): Conv2d(288, 288, kernel_size=(9, 9), stride=(2, 2), padding=(4, 4), groups=288, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(96, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (2): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)\n",
      "          (3): Conv2d(480, 480, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=480, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1920, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(160, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (2): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)\n",
      "          (3): Conv2d(480, 480, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=480, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1920, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(160, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv_pw): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (2): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)\n",
      "          (3): Conv2d(480, 480, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=480, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1920, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(160, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv_pw): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): MixedConv2d(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (2): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)\n",
      "          (3): Conv2d(480, 480, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=480, bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1920, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(160, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): MixedConv2d(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_head): Conv2d(320, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (global_pool): SelectAdaptivePool2d (output_size=1, pool_type=avg)\n",
      "  (classifier): Linear(in_features=1536, out_features=11, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 25\n",
    "test_split = 0.1\n",
    "\n",
    "#N_test_samples = round(test_split * len(train_data))\n",
    "#train_set, val_set = torch.utils.data.random_split(train_data,[len(train_data) - N_test_samples, N_test_samples])\n",
    "#p = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\images\\food-11\\evaluation\"\n",
    "#train_dl = torch.utils.data.DataLoader(trainDataset([img_dir[0]]), batch_size=batch_size,shuffle=True)\n",
    "#val_dl = torch.utils.data.DataLoader(valDataset([img_dir[1]]), batch_size=batch_size)\n",
    "#tmp = valDataset([p])\n",
    "#train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
    "#val_dl = torch.utils.data.DataLoader(valDataset([p]), batch_size=batch_size)\n",
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
    "val2_dl = torch.utils.data.DataLoader(val2_set, batch_size=batch_size)\n",
    "\n",
    "\n",
    "#train_set = train_data\n",
    "#val_set = tmp\n",
    "            \n",
    "            \n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "resnet50 = models.resnext50_32x4d(pretrained=True)\n",
    "set_parameter_requires_grad(resnet50, True)\n",
    "fc_features = resnet50.fc.in_features\n",
    "resnet50.fc = nn.Linear(fc_features, 11)\n",
    "\n",
    "#import timm\n",
    "\n",
    "resnet50 = timm.create_model('mixnet_xl', pretrained=True)\n",
    "set_parameter_requires_grad(resnet50, True)\n",
    "resnet50.classifier = nn.Linear(1536, 11)\n",
    "print(resnet50)\n",
    "#next(resnet50.layer4[1].conv1.parameters()).requires_grad = True\n",
    "#next(resnet50.layer4[1].conv2.parameters()).requires_grad = True\n",
    "#next(resnet50.layer4[1].conv3.parameters()).requires_grad = True\n",
    "\n",
    "#next(resnet50.layer4[2].conv1.parameters()).requires_grad = True\n",
    "#next(resnet50.layer4[2].conv2.parameters()).requires_grad = True\n",
    "#next(resnet50.layer4[2].conv3.parameters()).requires_grad = True\n",
    "\n",
    "#next(resnet50.layer4[0].conv1.parameters()).requires_grad = True\n",
    "#next(resnet50.layer4[0].conv2.parameters()).requires_grad = True\n",
    "#next(resnet50.layer4[0].conv3.parameters()).requires_grad = True\n",
    "\n",
    "#next(resnet50.layer3[22].conv1.parameters()).requires_grad = True\n",
    "#next(resnet50.layer3[22].conv2.parameters()).requires_grad = True\n",
    "#next(resnet50.layer3[22].conv3.parameters()).requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "LEARNING_RATE = 0.01\n",
    "net = resnet50\n",
    "net = net.cuda()\n",
    "N_EPOCHS = 50\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr = LEARNING_RATE, momentum=0.9,weight_decay=1e-4) \n",
    "# MILESTONE = [1, 20, 30, 40]\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONE, gamma=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = (N_EPOCHS // 9) + 1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c5b514c3d64453924259aaa97073ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=479), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 71.89% Training loss: 0.9311779392163788 learning rate: 0.01\n",
      "Validation accuracy:80.90% Validation loss: 0.024848806320276477\n",
      "Validation accuracy:83.93% Validation loss: 0.02266037957990458\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6754e832147e4a61a4c034dafa5878ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=479), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 81.11% Training loss: 0.6019644092356238 learning rate: 0.009504844339512096\n",
      "Validation accuracy:83.46% Validation loss: 0.02209740464848683\n",
      "Validation accuracy:85.21% Validation loss: 0.019767612103102346\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771b3efe519c44109c4eff06efc65d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=479), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-440a4ec0899e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Get a batch from the dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1015\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-cefe7a2a3a8e>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \"\"\"\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mhflip\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFLIP_LEFT_RIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mtranspose\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m   2398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2399\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meffect_spread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_val_loss = [] \n",
    "epoch_val_acc = []\n",
    "epoch_val2_loss = []\n",
    "epoch_val2_acc = []\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "best_val_acc = 0\n",
    "best_val2_acc = 0\n",
    "flag = 0\n",
    "for e in range(N_EPOCHS):\n",
    "    print(\"EPOCH:\",e)\n",
    "  \n",
    "    ### TRAINING LOOP\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "  \n",
    "    ## Put the network in training mode\n",
    "    net.train()\n",
    "    \n",
    "      \n",
    "    for i, batch in enumerate(tqdm_notebook(train_dl)):\n",
    "        # Get a batch from the dataloader\n",
    "        x = batch[0]\n",
    "        labels = batch[1]\n",
    "        # move the batch to GPU\n",
    "        x = x.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Compute the network output\n",
    "        y = net(x)\n",
    "        loss = criterion(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Apply one step of the descent algorithm to update the weights\n",
    "        optimizer.step()\n",
    "        ## Compute some statistics\n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "    \n",
    "    print(\"Training accuracy: {:.2f}%\".format(100*running_accuracy/float(len(train_set))),\n",
    "          \"Training loss:\", running_loss/float(len(train_dl)), \"learning rate:\", scheduler.get_lr()[0])\n",
    "    \n",
    "    scheduler.step()\n",
    "    epoch_train_acc.append(running_accuracy/float(len(train_set)))\n",
    "    epoch_train_loss.append(running_loss/float(len(train_set)))\n",
    "    \n",
    "    ### VALIDATION LOOP\n",
    "    net.eval()\n",
    "\n",
    "    running_val_loss = 0\n",
    "    running_val_accuracy = 0\n",
    "    \n",
    "    for i, batch in enumerate(val_dl):\n",
    "        with torch.no_grad():\n",
    "            # Get a batch from the dataloader\n",
    "            x = batch[0]\n",
    "            labels = batch[1]\n",
    "            x = x.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Compute the network output\n",
    "            y = net(x)\n",
    "            loss = criterion(y, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "    acc = running_val_accuracy/float(len(val_set))\n",
    "    if acc > best_val_acc:\n",
    "        best_val_acc = acc\n",
    "        torch.save(net.state_dict(), 'resnext50_imageAug.pkl')\n",
    "        flag = 0\n",
    "    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n",
    "          \"Validation loss:\", running_val_loss/float(len(val_set)))\n",
    "    epoch_val_loss.append(running_val_loss/len(val_set))\n",
    "    epoch_val_acc.append(running_val_accuracy/len(val_set))\n",
    "\n",
    "    \n",
    "    \n",
    "    running_val2_loss = 0\n",
    "    running_val2_accuracy = 0\n",
    "    \n",
    "    for i, batch in enumerate(val2_dl):\n",
    "        with torch.no_grad():\n",
    "            # Get a batch from the dataloader\n",
    "            x = batch[0]\n",
    "            labels = batch[1]\n",
    "            x = x.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Compute the network output\n",
    "            y = net(x)\n",
    "            loss = criterion(y, labels)\n",
    "\n",
    "            running_val2_loss += loss.item()\n",
    "            running_val2_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "    acc = running_val2_accuracy/float(len(val2_set))\n",
    "    if acc > best_val2_acc:\n",
    "        best_val2_acc = acc\n",
    "        torch.save(net.state_dict(), 'resnext50_imageAug2.pkl')\n",
    "        flag = 0\n",
    "    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n",
    "          \"Validation loss:\", running_val2_loss/float(len(val2_set)))\n",
    "    epoch_val2_loss.append(running_val2_loss/len(val2_set))\n",
    "    epoch_val2_acc.append(running_val2_accuracy/len(val2_set))\n",
    "    \n",
    "\n",
    "#Training accuracy: 61.29% Training loss: 1.310349090006745 learning rate: 0.0004999999999999924\n",
    "#Validation accuracy:78.43% Validation loss: 0.02216043197726369\n",
    "#EPOCH: 1 \n",
    "# LEARNING RATES TOO LOW \n",
    "\n",
    "#Training accuracy: 79.99% Training loss: 0.6488370027839173 learning rate: 6.698729810777737e-05\n",
    "#Validation accuracy:82.36% Validation loss: 0.016642803085137037\n",
    "#EPOCH: 2\n",
    "\n",
    "\n",
    "#resnext50\n",
    "#Training accuracy: 72.03% Training loss: 0.8572557178827432 learning rate: 0.001\n",
    "#Validation accuracy:83.88% Validation loss: 0.0037515729383372706\n",
    "#EPOCH: 1\n",
    "\n",
    "#Training accuracy: 94.75% Training loss: 0.17378924873012763 learning rate: 0.001\n",
    "#Validation accuracy:89.53% Validation loss: 0.0026218994257227674\n",
    "#EPOCH: 14\n",
    "\n",
    "\n",
    "\n",
    "#mixnet\n",
    "\n",
    "#Training accuracy: 99.87% Training loss: 0.0056521773303466446 learning rate: 0.006545084971874732\n",
    "#Validation accuracy:96.71% Validation loss: 0.0057580280123060035\n",
    "#EPOCH: 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "#Restarting training at the best validation accuracy epoch , with different scheduler and learning rates \n",
    "# mixnet best accuracy for pretraining 96.79%\n",
    "# restart with again cosine and after this one output the best model we will continue last training with multistep lr\n",
    "\n",
    "restart_training = timm.create_model('mixnet_xl', pretrained=True)\n",
    "restart_training.classifier = nn.Linear(1536, 11)\n",
    "restart_training.load_state_dict(torch.load('model_mixnet_val2.pkl'))\n",
    "\n",
    "root_dir = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\food-11\\evaluation\\\\\"\n",
    "train_dir = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\\training\\\\\"\n",
    "val_dir = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\food-11\\validation\\\\\"\n",
    "new_train, new_val = random_split_train_val(root_dir, val_dir, 0.1)\n",
    "train_set = trainDataset(r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\food-11\", new_train)\n",
    "print(train_set[0])\n",
    "print(len(train_set))\n",
    "val_set = valDataset(r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\", new_val)\n",
    "print(val_set[0])\n",
    "print(len(val_set))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
    "val2_dl = torch.utils.data.DataLoader(val2_set, batch_size=batch_size)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "LEARNING_RATE = 0.001\n",
    "restart_training.cuda()\n",
    "\n",
    "\n",
    "N_EPOCHS = 50\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "optimizer = torch.optim.SGD(restart_training.parameters(),lr = LEARNING_RATE, momentum=0.9,weight_decay=1e-4) \n",
    "MILESTONE = [1, 20, 30, 40]\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONE, gamma=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = (N_EPOCHS // 9) + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_val_loss = [] \n",
    "epoch_val_acc = []\n",
    "epoch_val2_loss = []\n",
    "epoch_val2_acc = []\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "best_val_acc = 0\n",
    "best_val2_acc = 0\n",
    "flag = 0\n",
    "for e in range(N_EPOCHS):\n",
    "    print(\"EPOCH:\",e)\n",
    "  \n",
    "    ### TRAINING LOOP\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "  \n",
    "    ## Put the network in training mode\n",
    "    restart_training.train()\n",
    "    \n",
    "      \n",
    "    for i, batch in enumerate(tqdm_notebook(train_dl)):\n",
    "        # Get a batch from the dataloader\n",
    "        x = batch[0]\n",
    "        labels = batch[1]\n",
    "        # move the batch to GPU\n",
    "        x = x.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Compute the network output\n",
    "        y = restart_training(x)\n",
    "        loss = criterion(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Apply one step of the descent algorithm to update the weights\n",
    "        optimizer.step()\n",
    "        ## Compute some statistics\n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "    \n",
    "    print(\"Training accuracy: {:.2f}%\".format(100*running_accuracy/float(len(train_set))),\n",
    "          \"Training loss:\", running_loss/float(len(train_dl)), \"learning rate:\", scheduler.get_lr()[0])\n",
    "    \n",
    "    scheduler.step()\n",
    "    epoch_train_acc.append(running_accuracy/float(len(train_set)))\n",
    "    epoch_train_loss.append(running_loss/float(len(train_set)))\n",
    "    \n",
    "    ### VALIDATION LOOP\n",
    "    restart_training.eval()\n",
    "\n",
    "    running_val_loss = 0\n",
    "    running_val_accuracy = 0\n",
    "    \n",
    "    for i, batch in enumerate(val_dl):\n",
    "        with torch.no_grad():\n",
    "            # Get a batch from the dataloader\n",
    "            x = batch[0]\n",
    "            labels = batch[1]\n",
    "            x = x.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Compute the network output\n",
    "            y = restart_training(x)\n",
    "            loss = criterion(y, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "    acc = running_val_accuracy/float(len(val_set))\n",
    "    if acc > best_val_acc:\n",
    "        best_val_acc = acc\n",
    "        torch.save(restart_training.state_dict(), 'model_mixnet_restart2.pkl')\n",
    "        flag = 0\n",
    "    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n",
    "          \"Validation loss:\", running_val_loss/float(len(val_set)))\n",
    "    epoch_val_loss.append(running_val_loss/len(val_set))\n",
    "    epoch_val_acc.append(running_val_accuracy/len(val_set))\n",
    "\n",
    "    \n",
    "    \n",
    "    running_val2_loss = 0\n",
    "    running_val2_accuracy = 0\n",
    "    \n",
    "    for i, batch in enumerate(val2_dl):\n",
    "        with torch.no_grad():\n",
    "            # Get a batch from the dataloader\n",
    "            x = batch[0]\n",
    "            labels = batch[1]\n",
    "            x = x.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Compute the network output\n",
    "            y = restart_training(x)\n",
    "            loss = criterion(y, labels)\n",
    "\n",
    "            running_val2_loss += loss.item()\n",
    "            running_val2_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "    acc = running_val2_accuracy/float(len(val2_set))\n",
    "    if acc > best_val2_acc:\n",
    "        best_val2_acc = acc\n",
    "        torch.save(restart_training.state_dict(), 'model_mixnet_val2_restart2.pkl')\n",
    "        flag = 0\n",
    "    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n",
    "          \"Validation loss:\", running_val2_loss/float(len(val2_set)))\n",
    "    epoch_val2_loss.append(running_val2_loss/len(val2_set))\n",
    "    epoch_val2_acc.append(running_val2_accuracy/len(val2_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "restart_training = timm.create_model('mixnet_xl', pretrained=True)\n",
    "restart_training.classifier = nn.Linear(1536, 11)\n",
    "restart_training.load_state_dict(torch.load('model_mixnet.pkl'))\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "LEARNING_RATE = 0.001\n",
    "restart_training.cuda()\n",
    "\n",
    "\n",
    "N_EPOCHS = 50\n",
    "\n",
    "batch_size = 25\n",
    "test_split = 0.1\n",
    "\n",
    "N_test_samples = round(test_split * len(train_data))\n",
    "train_set, val_set = torch.utils.data.random_split(train_data,[len(train_data) - N_test_samples, N_test_samples])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "p = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\images\\food-11\\evaluation\"\n",
    "tmp = valDataset([p])\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(valDataset([p]), batch_size=batch_size)\n",
    "\n",
    "\n",
    "train_set = train_data\n",
    "val_set = tmp\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(restart_training.parameters(),lr = LEARNING_RATE, momentum=0.9,weight_decay=1e-4) \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee90c9cfb1254f578dc0b862d42c8a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=479), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 99.53% Training loss: 0.017840567157789845 learning rate: 0.001\n",
      "Validation accuracy:99.85% Validation loss: 0.00021236590418309898\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20aed40783d4b3a9a05ec1bd6626ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=479), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 99.57% Training loss: 0.017373887183357833 learning rate: 0.0009045084971874737\n",
      "Validation accuracy:99.92% Validation loss: 0.00016884828118530375\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d72212ef4424bc898e95068a59fa88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=479), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 99.58% Training loss: 0.015941037162442086 learning rate: 0.0006545084971874737\n",
      "Validation accuracy:99.85% Validation loss: 0.0002641361776154656\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41dc5b68bbe41dfbed5ff4664c070d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=479), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-78b40a06462d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Get a batch from the dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1015\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_val_loss = [] \n",
    "epoch_val_acc = []\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "best_val_acc = 0\n",
    "flag = 0\n",
    "N_EPOCHS = 50\n",
    "for e in range(N_EPOCHS):\n",
    "    print(\"EPOCH:\",e)\n",
    "  \n",
    "    ### TRAINING LOOP\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "  \n",
    "    ## Put the network in training mode\n",
    "    restart_training.train()\n",
    "    \n",
    "      \n",
    "    for i, batch in enumerate(tqdm_notebook(train_dl)):\n",
    "        # Get a batch from the dataloader\n",
    "        x = batch[0]\n",
    "        labels = batch[1]\n",
    "        # move the batch to GPU\n",
    "        x = x.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Compute the network output\n",
    "        y = restart_training(x)\n",
    "        loss = criterion(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Apply one step of the descent algorithm to update the weights\n",
    "        optimizer.step()\n",
    "        ## Compute some statistics\n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "    \n",
    "    print(\"Training accuracy: {:.2f}%\".format(100*running_accuracy/float(len(train_set))),\n",
    "          \"Training loss:\", running_loss/float(len(train_dl)), \"learning rate:\", scheduler.get_lr()[0])\n",
    "    \n",
    "    scheduler.step()\n",
    "    epoch_train_acc.append(running_accuracy/float(len(train_set)))\n",
    "    epoch_train_loss.append(running_loss/float(len(train_set)))\n",
    "    \n",
    "    ### VALIDATION LOOP\n",
    "    restart_training.eval()\n",
    "\n",
    "    running_val_loss = 0\n",
    "    running_val_accuracy = 0\n",
    "    \n",
    "    for i, batch in enumerate(val_dl):\n",
    "        with torch.no_grad():\n",
    "            # Get a batch from the dataloader\n",
    "            x = batch[0]\n",
    "            labels = batch[1]\n",
    "            x = x.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Compute the network output\n",
    "            y = restart_training(x)\n",
    "            loss = criterion(y, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "    acc = running_val_accuracy/float(len(val_set))\n",
    "    if acc > best_val_acc:\n",
    "        best_val_acc = acc\n",
    "        torch.save(restart_training.state_dict(), 'model_mixnet_restart1.pkl')\n",
    "        flag = 0\n",
    "    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n",
    "          \"Validation loss:\", running_val_loss/float(len(val_set)))\n",
    "    epoch_val_loss.append(running_val_loss/len(val_set))\n",
    "    epoch_val_acc.append(running_val_accuracy/len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3hc1bnv8e8rjaSR1ZvlIhe54YYLNqY4lGCKTTMEA+ZAwAQOJwQSkpsGJ50bcpJzcgPJCSGBACEEMI6Jg5M4kGAMDs0N3KvckFwkWbLVrDaa9/6xtuSxUBnZHhXr/TzPPDOz26zZsvdv1l57ryWqijHGGBOuqK4ugDHGmJ7FgsMYY0yHWHAYY4zpEAsOY4wxHWLBYYwxpkMsOIwxxnSIBYcxxpgOseAw3Z6I7BGRahGpDHkMEJE4EfkvEfnYm79DRL4uItJs/atFZKWIVIlIiYi8ICI5IfPniUhDyLZ3i8izIjKq2XaeFJFtIhIUkXnN5o0XkddF5JCItHtzlIh8X0Tqvc87IiLvich5J7mrGrerInJjyDSfN21oGOtfLCIFzaY13z+VInJxyPyhIrJMRI6KyFYRufRkv4fp3iw4TE9xjaomhjz2A38EZgBXAknAZ4F7gJ83riQic4AXvWmZwDigFnhHRNJCtv++qiYCKcClQDWwRkTGhyyzDvgC8GEL5asHFgB3deA7vex9ZiawzPs+p0Ip8LCIRJ+i7YG3f0Ieb4XMewn4CMgAvgUsFJGsU/jZppux4DA9kojMAC4HblDVjaoaUNUPgNuA+0RkhFfz+H/AD1X1BVWtVtWDwN1AJfCV5ttV1QZV3amqXwDeBr4fMu9xVV0K1LSw3jZVfRrY1NHvoqoB4AVgYOgB16sprQ2pkUwImfdNEdknIhVeLWhGyCZfA+q8ffEJXk3tp15NrVBEfi0i8SKSAPwdGBBas2ur7F6t7Czge97+fQXYANzQ0f1geg4LDtNTXQasUNX80ImqugIowNVEzgAG0+yXvKoGgVe8bbTlT8AFp6rArRGRWOB2oAQ47E07C3gG+A/cL/nfAIu9g/4ZwP3A2aqaBFwB7AnZpALfAb4nIjEtfORPgFHAJGAEMBD4rqpWAbOA/c1qdgCTvdNw20XkOyLi86aPA3apakXI9td5081pyoLD9BR/9n55HxGRP+NO7xxoZdkD3vzMkPetLdOW/UD6iRQ2TDeJyBHcabF/B+Z4tQ+8979R1RVeLeg53Cm2c4EGIA4YKyIxqrpHVXeGblhVFwPFuNpVE68W9u/AV1S11Dvg/wiY20Y5lwPjgb64msQtwNe9eYlAWbPly3CnDs1pyoLD9BTXqWqq97gOOAT0b2XZ/t78QyHvW1umLQNx7QUnRURuDTn18/eQWQtUNRXIBjYCU0LmDQG+GhKWR4BBwABVzQO+jDuNViQi81s5pfRtXJuDP2RaFtAH137TuN3XvOktUtVdqrpbVYOqugF4GJjjza4EkputkgxUYE5bFhymp3oDOEdEBoVOFJFpuAPsm8A23GmrG5stE4X75by0nc+4HvjXyRbUa19pPPUzq4X5h3CnpL4vIo0hlw88EhKWqaraR1Vf8tZ5UVU/hQsYxZ1+ar7dfwJ5uAb9RodwNZxxIdtN8Rrp8bbV7lcCGq9c2wQME5HQGsZETqCtx/QcFhymR1LVN3AH/ldEZJyIRIvIubhG5idUdYe6MQO+BnxbRP7NawDuB/wW96v40ebb9baTKyL/C1wM/CBkXqyI+HEHzRgR8XshhDh+INZ77xeRuA58n63A68A3vElPAZ8XkXO8bSeIyFUikiQiZ4jIJd72a3BB0NDKpr8Vss3G9p2ngEdFpK9X1oEicoW3SCGQISIpId97lohke69H49pPXvW2tx1Yi2tP8YvI9cAEXBuSOU1ZcJie7AbcZayv4U6Z/AF4Gvhi4wKq+jLuMt2v4H5tbwbigemqWhKyrfNEpBIoB97CBcvZ3qmZRv/AHaTPB570Xl/ozRvivW/8pV2Nq/F0xP8A94hIX1VdjWuL+CWuwTwPmOctFwf82Ps+B3FtD//Z0gZV9V1gZbPJ3/S294GIlONqb2d4y2/FXV67yzuVNQB3ocF6EakCluAuGvhRyPbmAlO9cv4Y11ZT3MHvbnoQsYGcjDHGdITVOIwxxnSIBYcxxpgOseAwxhjTIRYcxhhjOsTX/iI9X2Zmpg4dOrSri2GMMT3GmjVrDqlqizeG9orgGDp0KKtXr+7qYhhjTI8hIntbm2enqowxxnSIBYcxxpgOseAwxhjTIRFt4xCRmbiR16KB36rqj5vNjwN+j+sVtAS4WVX3ePMewo2m1gB8SVVf96an4voaGo/rbO1zqvp+JL+HMSZ89fX1FBQUUFPzifGuTDfk9/vJyckhJqaloVtaFrHg8IatfBw3WE4BsEpEFqvq5pDF7gIOq+oIEZmL6+HzZhEZi+v/ZhwwAHhDREapagMuiF5T1TneADh9IvUdjDEdV1BQQFJSEkOHDkWOH/7ddDOqSklJCQUFBeTm5oa9XiRPVU0D8ry+/OuA+cDsZsvMBp7zXi8EZngDzcwG5qtqraruxnXINk1EknGdyj0NoKp1qnokgt/BGNNBNTU1ZGRkWGj0ACJCRkZGh2uHkQyOgbgxBRoVeNNaXMYb+awMN0xma+sOw41q9qyIfCQiv/XGSf4EEblHRFaLyOriYuuo05jOZKHRc5zI3yqSwdFSaZp3xdvaMq1N9wFn4cZbmAxUAQ+29OGq+qSqTlXVqVlZrQ5u1qZfLN3B29stdIwxJlQkg6MANxJboxzcGM4tLiMiPiAFN1Rna+sWAAWqusKbvhAXJBHx5PJdvL3NgsMYY0JFMjhWASO90dRicY3di5stsxi4w3s9B3jTG7VtMTBXROJEJBcYCaxU1YNAvoic4a0zAzcwT0Qk+X1U1NRHavPGmAg4cuQIv/rVrzq83pVXXsmRIx1vMp03bx4LFy7s8Ho9WcSCw2uzuB83HOYWYIGqbhKRh0XkWm+xp3HDVOYB/wfvtJOqbgIW4ELhNeA+74oqcKO7vSAi64FJHD8S2SmV7I+h3ILDmB6lteBoaGhtdF1nyZIlpKamRqpYp5WI3sehqktwQ02GTvtuyOsa4MZW1n0EeKSF6Wtxw1RGnKtxBDrjo4w5Lf3gL5vYvL/8lG5z7IBkvnfNuFbnP/jgg+zcuZNJkyYRExNDYmIi/fv3Z+3atWzevJnrrruO/Px8ampqeOCBB7jnnnuAY33aVVZWMmvWLD71qU/x3nvvMXDgQF599VXi4+PbLdvSpUv52te+RiAQ4Oyzz+aJJ54gLi6OBx98kMWLF+Pz+bj88sv56U9/yh//+Ed+8IMfEB0dTUpKCsuXLz9l+yjSekUnhycqOT6Gogq7icmYnuTHP/4xGzduZO3atbz11ltcddVVbNy4sek+hWeeeYb09HSqq6s5++yzueGGG8jIyDhuGzt27OCll17iqaee4qabbuKVV17htttua/Nza2pqmDdvHkuXLmXUqFHcfvvtPPHEE9x+++0sWrSIrVu3IiJNp8MefvhhXn/9dQYOHHhCp8i6kgVHG5L8PnYWW43DmBPVVs2gs0ybNu24m9t+8YtfsGjRIgDy8/PZsWPHJ4IjNzeXSZMmATBlyhT27NnT7uds27aN3NxcRo0aBcAdd9zB448/zv3334/f7+fuu+/mqquu4uqrrwZg+vTpzJs3j5tuuonPfOYzp+Krdhrrq6oNyf4YyqutjcOYniwh4ditXm+99RZvvPEG77//PuvWrWPy5Mkt3vwWFxfX9Do6OppAoP0fkO66nk/y+XysXLmSG264gT//+c/MnDkTgF//+tf88Ic/JD8/n0mTJlFSUtLRr9ZlrMbRhsY2DlW1G5qM6SGSkpKoqKhocV5ZWRlpaWn06dOHrVu38sEHH5yyzx09ejR79uwhLy+PESNG8Pzzz3PRRRdRWVnJ0aNHufLKKzn33HMZMWIEADt37uScc87hnHPO4S9/+Qv5+fmfqPl0VxYcbUiOjyEQVKrrG+gTa7vKmJ4gIyOD6dOnM378eOLj48nOzm6aN3PmTH79618zYcIEzjjjDM4999xT9rl+v59nn32WG2+8salx/POf/zylpaXMnj2bmpoaVJVHH30UgK9//evs2LEDVWXGjBlMnDjxlJUl0qS16tXpZOrUqXoiIwC+sGIv31q0kRX/OYPsZH8ESmbM6WfLli2MGTOmq4thOqClv5mIrFHVFq9gtTaONiT5XTfDdhOgMcYcY+df2pDsd7unrNqurDKmt7vvvvt49913j5v2wAMPcOedd3ZRibqOBUcbrMZhjGn0+OOPd3URug07VdWGlHiXq+V297gxxjSx4GiD1TiMMeaTLDjakOwFR7m1cRhjTBMLjjb4Y6LwRYnVOIwxJoQFRxtEhCS/z7pWN+Y0lpiYCMD+/fuZM2dOi8tcfPHFtHcv2GOPPcbRo0eb3p/o+B6t6U7jflhwtCM5Psa6VjemFxgwYMBJHZibB8fpPL6HXY7bDhuTw5iT8PcH4eCGU7vNfmfCrB+3Ovub3/wmQ4YM4Qtf+AIA3//+9xERli9fzuHDh6mvr+eHP/whs2fPPm69PXv2cPXVV7Nx40aqq6u588472bx5M2PGjKG6urppuXvvvZdVq1ZRXV3NnDlz+MEPfsAvfvEL9u/fz6c//WkyMzNZtmxZ0/gemZmZ/OxnP+OZZ54B4O677+bLX/4ye/bs6bHjfliNox3WQ64xPcvcuXN5+eWXm94vWLCAO++8k0WLFvHhhx+ybNkyvvrVr7bamy3AE088QZ8+fVi/fj3f+ta3WLNmTdO8Rx55hNWrV7N+/Xrefvtt1q9fz5e+9CUGDBjAsmXLWLZs2XHbWrNmDc8++ywrVqzggw8+4KmnnuKjjz4C3Lgf9913H5s2bSI1NZVXXnml3e/XOO7Hyy+/zIYNGwgEAjzxxBOUlpayaNEiNm3axPr16/n2t78NHBv3Y926dSxe3Hz07hNjNY52JPl97Dl0tP0FjTGf1EbNIFImT55MUVER+/fvp7i4mLS0NPr3789XvvIVli9fTlRUFPv27aOwsJB+/fq1uI3ly5fzpS99CYAJEyYwYcKEpnkLFizgySefJBAIcODAATZv3nzc/Obeeecdrr/++qbu3T/zmc/wr3/9i2uvvbbHjvthNY522LjjxvQ8c+bMYeHChbz88svMnTuXF154geLiYtasWcPatWvJzs5ucRyOUC0NpbB7925++tOfsnTpUtavX89VV13V7nbaqtn01HE/LDjakeS3xnFjepq5c+cyf/58Fi5cyJw5cygrK6Nv377ExMSwbNky9u7d2+b6F154IS+88AIAGzduZP369QCUl5eTkJBASkoKhYWF/P3vf29ap7VxQC688EL+/Oc/c/ToUaqqqli0aBEXXHDBCX+30HE/gOPG/SgrK+PKK6/kscceY+3atcCxcT8efvhhMjMzyc/PP+HPbmSnqtqRHO+jsjZAQ1CJjrLBnIzpCcaNG0dFRQUDBw6kf//+3HrrrVxzzTVMnTqVSZMmMXr06DbXv/fee7nzzjuZMGECkyZNYtq0aQBMnDiRyZMnM27cOIYNG8b06dOb1rnnnnuYNWsW/fv3P66d46yzzmLevHlN27j77ruZPHlyWKelWtIdxv2w8Tja8fQ7u/m/f93Muu9eTkqfmFNcMmNOPzYeR89j43GcYkn+xo4OrZ3DGGPATlW1q6m/KgsOY0wn6AnjflhwtKNxMCdrIDcmfKra4lVJpn2dPe7HiTRX2KmqdiTHN/aQazUOY8Lh9/spKSk5oQOS6VyqSklJCX6/v0PrWY2jHUlW4zCmQ3JycigoKKC4uLiri2LC4Pf7ycnJ6dA6FhztsDYOYzomJiaG3Nzcri6GiaCInqoSkZkisk1E8kTkwRbmx4nIy978FSIyNGTeQ970bSJyRcj0PSKyQUTWisiJXWPbAYlW4zDGmONErMYhItHA48BlQAGwSkQWq+rmkMXuAg6r6ggRmQv8BLhZRMYCc4FxwADgDREZpaoN3nqfVtVDkSp7qJjoKOJjoq2NwxhjPJGscUwD8lR1l6rWAfOB2c2WmQ08571eCMwQdynGbGC+qtaq6m4gz9tel0iOt67VjTGmUSSDYyAQ2ilKgTetxWVUNQCUARntrKvAP0RkjYjc09qHi8g9IrJaRFafbCNdkj+GilqrcRhjDEQ2OFq6iLv59XmtLdPWutNV9SxgFnCfiFzY0oer6pOqOlVVp2ZlZYVb5hYl+32UV1uNwxhjILLBUQAMCnmfA+xvbRkR8QEpQGlb66pq43MRsIhOOIXlesi1GocxxkBkg2MVMFJEckUkFtfY3Xz4qcXAHd7rOcCb6u4aWgzM9a66ygVGAitFJEFEkgBEJAG4HNgYwe8AuJsAy62NwxhjgAheVaWqARG5H3gdiAaeUdVNIvIwsFpVFwNPA8+LSB6upjHXW3eTiCwANgMB4D5VbRCRbGCR15WBD3hRVV+L1Hdo5MYdtxqHMcZAhG8AVNUlwJJm074b8roGuLGVdR8BHmk2bRdw8p3Jd5Abd9xqHMYYA9ZXVViS/D7qGoLU1De0v7AxxpzmLDjCkGxjchhjTBMLjjA09pBrNwEaY4wFR1ish1xjjDnGgiMMTT3kWn9VxhhjwRGOJL+dqjLGmEYWHGFIjrfGcWOMaWTBEYZjNQ4LDmOMseAIQ0JsNFGC3QRojDFYcIRFRKyjQ2OM8VhwhCnJ77OODo0xBguOsCVbjcMYYwALjrBZjcMYYxwLjjAlx8fYDYDGGIMFR9jcmBxW4zDGGAuOMCX7Y+wGQGOMwYIjbMl+H5W1AYJB7eqiGGNMl7LgCFOSPwZVqKyz01XGmN7NgiNMjf1VWTuHMaa3s+AIU5J1rW6MMYAFR9iSrWt1Y4wBLDjCdmwUQKtxGGN6NwuOMDWOO26X5BpjejsLjjDZuOPGGONYcISpMTiscdwY09tZcLQmGIR9a6BkJwBxvmjifFFW4zDG9HoWHG159kpY/UzT2yTrdsQYYyw4WhUVBWm5ULq7aVJyvHWtbowxEQ0OEZkpIttEJE9EHmxhfpyIvOzNXyEiQ0PmPeRN3yYiVzRbL1pEPhKRv0ay/KTnQumuprdu+FgLDmNM7xax4BCRaOBxYBYwFrhFRMY2W+wu4LCqjgAeBX7irTsWmAuMA2YCv/K21+gBYEukyt4kfRgc3u3aO3AdHVrjuDGmt4tkjWMakKequ1S1DpgPzG62zGzgOe/1QmCGiIg3fb6q1qrqbiDP2x4ikgNcBfw2gmV30nMhUAOVBwEbPtYYYyCywTEQyA95X+BNa3EZVQ0AZUBGO+s+BnwDCLb14SJyj4isFpHVxcXFJ/YN0oe5Z+90lbVxGGNMZINDWpjWfDCL1pZpcbqIXA0Uqeqa9j5cVZ9U1amqOjUrK6v90rYkLdc9e8GRZDUOY4yJaHAUAINC3ucA+1tbRkR8QApQ2sa604FrRWQP7tTXJSLyh0gUHoCUQRDlOxYccT5q6oPUBdqs7BhjzGktksGxChgpIrkiEotr7F7cbJnFwB3e6znAm6qq3vS53lVXucBIYKWqPqSqOao61Nvem6p6W8S+QbQPUoc0XZLb2F+V1TqMMb2ZL1IbVtWAiNwPvA5EA8+o6iYReRhYraqLgaeB50UkD1fTmOutu0lEFgCbgQBwn6o2RKqsbQq5JLep25GaABmJcV1SHGOM6WoRCw4AVV0CLGk27bshr2uAG1tZ9xHgkTa2/Rbw1qkoZ5vSh8HHK0A1ZEwOq3EYY3ovu3O8PenDoK4Cqg5ZD7nGGIMFR/saL8k9vPvYmBx2E6Axphez4GhPyCW5VuMwxhgLjvalDQHECw4bBdAYYyw42uOLc/dzlO4iKc6HCHb3uDGmV7PgCEe66149KkpIjLWODo0xvZsFRzhC7uVIjreu1Y0xvZsFRzjSh0F1KVQfJsnvszYOY0yvZsERjqZecndb1+rGmF7PgiMcIfdyJPl9dqrKGNOrWXCEI22oey7dRXJ8jJ2qMsb0ahYc4YhNgMR+UGo1DmOMseAIV/qw44LD9f5ujDG9jwVHuNKHuVNV/hgagsrRuq7p5d0YY7qaBUe40odC5UFSfa59w9o5jDG9VVjBISIPiEiyOE+LyIcicnmkC9eteFdW9QseAKyjQ2NM7xVujeNzqloOXA5kAXcCP45YqbojLzgy6tyw6dbtiDGmtwo3OMR7vhJ4VlXXhUzrHbzu1dNq8gGrcRhjeq9wg2ONiPwDFxyvi0gSEIxcsbqh+FSITyex6mPA2jiMMb1XuGOO3wVMAnap6lERScedrupd0ocRX7EXsK7VjTG9V7g1jvOAbap6RERuA74NlEWuWN1U+jBiyl1wWH9VxpjeKtzgeAI4KiITgW8Ae4HfR6xU3VV6LpQVkBAdoLzaahzGmN4p3OAIqLtVejbwc1X9OZAUuWJ1U+nDEJTRcYetxmGM6bXCbeOoEJGHgM8CF4hINBATuWJ1U94luaNiiqyNwxjTa4Vb47gZqMXdz3EQGAj8T8RK1V15wTHMV2w1DmNMrxVWcHhh8QKQIiJXAzWq2vvaOPpkQGwSgzlo93EYY3qtcLscuQlYCdwI3ASsEJE5kSxYtyQC6bnk6EG7c9wY02uFe6rqW8DZqnqHqt4OTAO+095KIjJTRLaJSJ6IPNjC/DgRedmbv0JEhobMe8ibvk1ErvCm+UVkpYisE5FNIvKDMMt/6qQPIzuwz2ocxpheK9zgiFLVopD3Je2t6zWgPw7MAsYCt4jI2GaL3QUcVtURwKPAT7x1xwJzgXHATOBX3vZqgUtUdSLuhsSZInJumN/h1EgfRlr9Qapqajr1Y40xprsINzheE5HXRWSeiMwD/gYsaWedaUCequ5S1TpgPu5y3lCzgee81wuBGSIi3vT5qlqrqruBPGCaOpXe8jHeo3NHVErPJVobSK0vJNDQu3pdMcYYCL9x/OvAk8AEYCLwpKp+s53VBgL5Ie8LvGktLqOqAdzd6BltrSsi0SKyFigC/qmqK8L5DqeMd2XVUCm001XGmF4p3Ps4UNVXgFc6sO2Wes9tXjtobZlW11XVBmCSiKQCi0RkvKpu/MSHi9wD3AMwePDgDhS7HV5wDPGCIy0h9tRt2xhjeoD22ikqRKS8hUeFiJS3s+0CYFDI+xxgf2vLiIgPSAFKw1lXVY8Ab+HaQD5BVZ9U1amqOjUrK6udonZAYj8aov0MkULrIdcY0yu1GRyqmqSqyS08klQ1uZ1trwJGikiuiMTiGrsXN1tmMXCH93oO8KbXtcliYK531VUuMBJYKSJZXk0DEYkHLgW2duQLn7SoKGoTBzPUgsMY00uFfaqqo1Q1ICL3A68D0cAzqrpJRB4GVqvqYuBp4HkRycPVNOZ6624SkQXAZiAA3KeqDSLSH3jOu8IqCligqn+N1HdoTSB1KEMOb2K3tXEYY3qhiAUHgKouodnVV6r63ZDXNbibClta9xHgkWbT1gOTT31JOyh9GIP3LGPd0dquLokxxnS6cC/HNSF8mcPwSz0NZQe6uijGGNPpLDhOQFzfEQD4juzu4pIYY0zns+A4AdGZwwGI84aRNcaY3sSC40Qk51CPj4Sqj7u6JMYY0+ksOE5EtI/CqL6kVOe3v6wxxpxmLDhOUJFvABl1+7q6GMYY0+ksOE7QobhBDKzfC9tf7+qiGGNMp7LgOEHvZN3C3qgcePEm+Md3oMHuIjfG9A4WHCeoIWkAt/EjmPo5eO8X8OyVcMTaPIwxpz8LjhM0IDWewmrYfvbDcMPTULQZfnMBbHutq4tmjDERZcFxgm6ZNpjEWB///do2OHMO/MdySM6Bl262U1fGmNOaBccJSk+I5fMXD+eNLYWs2lMKGcPh7jdg6l3HTl1tXgyVRe1vzBhjehBxvZif3qZOnaqrV68+5dutrmvgov9ZRk5aPK/cez5u1Ftg4yvwl69AbZl7n5YLg8+FQee458wzIMoy2xjTfYnIGlWd2tK8iPaOe7qLj43mK5eN4qE/beAfmwu5Ylw/N2P8DTD6ajiwDj7+APJXwI5/wrqX3Hx/Koy5Gi77v9Anveu+gDHGnACrcZykQEOQKx5bDsDrX74QX3QrNQlVKN3lgmTve7B+PvTJgGt+AWe0OIihMcZ0mbZqHHa+5CT5oqP4xszR7Cyu4o9rClpfUMS1g0y+Fa57HP79TeiT6RrTF90L1Uc6r9DGGHMSLDhOgcvHZjNlSBqP/nM7R+vCHBWw/0S45y244Guw/mV44nzIeyOSxTTGmFPCguMUEBEemjWaoopann13T/gr+mJhxnfg7n9CbCL84Qb4ywNQWxGxshpjzMmyxvFTZOrQdC4bm82v39rJLdMGk54QG/7KA6e4+0CWPQLv/a+7iTBzJMTEg88f/rPPD/5kyBgJCRmR+7LGmF7NGsdPoR2FFVzx2HLmnZ/Ld68Ze2Ib+XgFvPMo1JRB/VEI1EB9tfdc46YFw7i5sE8mZI2GvqPdc9YZ7jmx74mVyxjTq9jluJ1kZHYSN00dxPMf7OHO6UMZlN6n4xsZfA782/y2lwk2HAuSQLX37D2OlsKh7VC8FYq3wfo/HrufBGDAZDj7bhj3GYg9gfIZY3o9q3GcYgfLarj4p8uYOa4fj82d3Cmf2SZVqDjoguTgelj7onvtT4FJt7lOGjNHdHUpjTHdjF2O24n6pfj53PRc/rx2Pyt2lXR1cdxlwMn9YfinYfoD8IUPYN7fYPgMWPkb+OUU+P1s2PIXaAi5IkzVq9nUeafIarruO3Qnh/Jg69+g7mhXl8SYLmM1jggoq67n2l++w8GyGh69eRJXntm/0z67QyoK4aPfw+rfQXkBSLSbrg0tL588EPqdefwjdWj36j4lUAtVxa6PsMbnysKQ10VQVeTum8k6w3UBM/hcyDnb1cKaq6+Bve/Cjn+4R+kuN71PJpzzeZh2N8Snde53NKYTtFXjsOCIkJLKWu55fg1r9h7mGzPP4N6Lhh/ry6q7aQi4g2LBKpAoiIp2z6GPYINrOzm4wT03hktsEvQb7xre03Ndv1yNz3GJkSlv48F81zI48jFUFrswqGmUEEIAABoYSURBVCp2FxW0JC4ZErIgMRsSsyAuyX2XgxtAg4BA9rhj/YnVVbl9sustd0GCzw+5F8LIyyFtKKx8Cna87i6jnjIPzrsPkgdE5vsa0wUsOLogOABq6hv4+sL1/GXdfm6eOogfXj+emNa6JOlJ6quhaItrMzm4AQ6sd2FS0+zu94QsFyBpQyA+HeJT3a96f4rrr6vxdUKmWzY6pvXPPLwX8v7p+vzavdwdzKPj3LYTsrxQ6AsJfd32Gl8neo+Y+Ja3W1sJ+1a7rmA+/gAKVkOddx9NymAYdTmMvAKGfuqTFxMc3Ajv/tx1ailRMHGuOx2YOfLE9mttJQQDEOVz+yIq5vjaXDDowrFiP5QfCHk+AFWHXM0nqR8k9feevUdiP4jxn1iZQqm6zyra4h7FW1wNb9RMGHWFC2MTearuh01tOdSUe3/37FP+MRYcXRQcAMGg8ugb2/nfN/OYPiKDX906hZT4Ng6QPVn1YTi8B0p3w+Hd3vMeVyuoOeL+kdPavzdxfXclZnsH+2z3n6EhADuXugZ9gNQh7iA14rKWD+YnK9jgBuWKinGnssKpJR7eA+/9Ej563h1Is86AlEGQOhhSB4W8HgyxCW6/lO6Ekjwo2eW93glHD7WwcTkWIg21LliOmx3t9lVCBlSXuQN7S5drpw6GIdNh8HnuOWN429+tstjth8aAKNoCRVuPv0KvT6bbRlWxC/ERl8K469zfp6XTfqHqjroDH7gDYZPG1+L+trGJrgbcmppyKMt3o2+W5R/7t5Y6GDJGQPpwSB/Wcu237uixKxAbw7B8PzTUeY/6419rg9vmgEnu6sT+k10ttaOhHGxw/1eqDsHREvd3rylzPxxqK9wPl9oK977Om1ZT7vZ9Tbl7H3o6+ZJvw4Vf71gZwmDB0YXB0WjhmgIe+tN6hmQk8Oy8s0/sUt2eLhj0fiWVeUFS5toajh461hZRUeieG9+j7kA38jJ3mihjRHgH865QWQyrn3E1sSMfuwNZ9eG210nq7w5uGcNc7czndwf+hnp3gAnWu7BoqIfoWHc6LKm/u+AhaYAL2dADazDoPrPigLuarvKgq5UcXAd73z8WTgl9Ych5MPh8d69P6W7v4OmFRWiIxadB1hi3XN+x3v1BY1zNLtjgen/e/Kobf6Zivyvn8Etg9FWuJla+H8r3ueeyfe5189ppW2K8AIlL9J6T3L+jI/mf3E50nLsJtqq45f2cPtRdsl60xQV+Y1BFxbiaYuoQ8MW57xAdC9G+Y6/Brbf/I6gu9dbzuX0yYJL7ezReHh96D1Z9tashVJe6sKg+TOs/oABf/PHfNS7JnWr1J7f83G8CZI0Kf3+GqcuCQ0RmAj8HooHfquqPm82PA34PTAFKgJtVdY837yHgLqAB+JKqvi4ig7zl+wFB4ElV/Xl75egOwQHw/s4SPv+HNfiihKfumMpZg61RtU2NV3ZF9+DbjWorjv81XFvh2oDa+iUcKapwaIdrH/r4fddLc1n+sfkxCV44jHEHw75j3evE7PDCOhh0p/02v+oeodvukwkpA90FFskD3MOfGrLdkO2LuHanuqPHfnHXVYb8Aq90tZHUwV5tbpA74KcMcqcso6LcMqUhtbnSXa6GV7rbBWHzEEwf1vap0ub7sSzfBcj+td7zRy7EouNcDSSmz7FeHWLi3fs+6a5W3SfThW6fDPdIyHT7Ii7RtRl2k3/vXRIcIhINbAcuAwqAVcAtqro5ZJkvABNU9fMiMhe4XlVvFpGxwEvANGAA8AYwCugL9FfVD0UkCVgDXBe6zZZ0l+AA2Flcyed+t4rC8hp+ectZXDr21J+bNCZsRz52YZIx3LXpnKor5FTdKSCf3/3aPxVtLN2Zqgu7tk6r9TBddR/HNCBPVXepah0wH5jdbJnZwHPe64XADHGXHs0G5qtqraruBvKAaap6QFU/BFDVCmALMDCC3+GUG56VyJ/uPZ9R2Un8xx/WsGB1fvsrGRMpqYNhxAx3pdipvKxaxPsln3v6hwa473sahUZ7IhkcA4HQo2IBnzzINy2jqgGgDMgIZ10RGQpMBla09OEico+IrBaR1cXFxS0t0mUyEuN48d/P5fzhGXxj4Xp+9VYevaGtyRhzeohkcLR0UrT50bG1ZdpcV0QSgVeAL6tqeUsfrqpPqupUVZ2alZUVZpE7T2Kcj6fvOJtrJw7gv1/bxsN/3UwwaOFhjOn+ItkKUwAMCnmfA+xvZZkCEfEBKUBpW+uKSAwuNF5Q1T9FpuidI9YXxWM3TyIjMZZn391DSWUdP71xIrG+0+BeD2PMaSuSR6hVwEgRyRWRWGAusLjZMouBO7zXc4A31Z2zWQzMFZE4EckFRgIrvfaPp4EtqvqzCJa900RFCd+9eizfnDmaxev287nfraKyNsxRBI0xpgtELDi8Nov7gddxjdgLVHWTiDwsItd6iz0NZIhIHvB/gAe9dTcBC4DNwGvAfaraAEwHPgtcIiJrvceVkfoOnUVEuPfi4fz3nAm8v6uEW578gMJy61TQGNM92Q2A3czSLYXc/+JHJMT5+OW/TebcYTaSnzGm81m36j3IjDHZvHr/dJL9Pm797QqeWr7LrrgyxnQrFhzd0KjsJF69fzqXjunLI0u2cN+LH1q7hzGm27Dg6KaS/DH8+rYpPDRrNK9tPMjsX75DXlFFVxfLGGMsOLozEeE/LhrOH+4+xxsc6l3+ur75Fc3GGNO5LDh6gPOHZ/LXL17A6H5J3P/iR9z34oe8s+MQDXbDoDGmC3SPbhhNu/ql+Jl/z3k8+sZ2XvhgL39bf4ABKX4+c1YON0zJITczoauLaIzpJexy3B6opr6BN7YUsnBNAcu3FxNUmDokjTlTcrhqQn+S/KfpQFHGmE5jAzmdZsERqrC8hkUf7eOPq/PZWVxFYpyP710zljlTcrrvGOfGmG7PguM0Do5Gqsra/CP819+3snJ3KbPG9+NH159JWkJsVxfNGNMD2Q2AvYCIMHlwGi/9+7k8OGs0b2wp5IrHlrN8e/fqUt4Y0/NZcJxmoqOEz180nEVfmE5yfAy3P7OS7y/eRE19Q/srG2NMGCw4TlPjB6bw1y9+innnD+V37+3hmv99h037y7q6WMaY04AFx2nMHxPN968dx3Ofm8aR6nque/xdvrVoA2vzj1j/V8aYE2aN471EaVUd/7VkC4vX7ac2EGRE30TmTMnh+skDyU7uBWNCG2M6xK6qsuBoUl5Tz5L1B1i4poDVew8TJXDhqCzmTMnh0jHZ+GOiu7qIxphuwILDgqNFu4or+dOH+3jlwwIOlNUQGx3FsKwERmUnMSo70XtOYlB6H6Kj7J4QY3oTCw4LjjY1BJX3dh7inbxD7CisZNvBCvYdqW6aH+eLYlR2ErdMG8xNU3PwRVvTmDGnOwsOC44Oq6wNkFdUyfbCCrYfrGDlnlLWF5QxLDOBr11xBrPG97M70405jbUVHNbJoWlRYpyPSYNSmTQoFXB3pr+xpYj/eX0rX3jhQybkpPDNmaOZPiKzi0tqjOlsds7BhEVEuGxsNn9/4EJ+euNESirruPW3K/js0yvYUGD3hxjTm9ipKnNCauobeGHFx/zyzR0cPlrP1RP6840rRjM4o09XF80YcwpYG4cFR8RU1NTz1PJdPPWv3QSCQW4/byhfvGQEqX2sc0VjejILDguOiCssr+Fn/9jOH9fkkxjn44uXjOT284cQ57P7Qozpiax3XBNx2cl+fjJnAkseuIDJg9N4ZMkWLv3Z2yxet9+6NzHmNGM1DhMR/9pRzI+WbGXLgXLGDUjm/OEZjOybxIjsREb0TSS5nVEKVZWK2gACNqKhMV3ATlVZcHSJhqCy6KN9PPvubnYUVVIXCDbN65fsZ6QXIr4ooaSyjkNVdZRU1lJaVUdJZR11DUFio6P4zjVjue2cwXbfiDGdyILDgqPLNQSV/NKj7CiqZEdRBXmFlewoqiSvqJKgKpmJcWQmxpKRGEdGgnvOTIxl+Y5DLN9ezOxJA/jR9WeSEGe3HhnTGSw4LDi6rcZ/f63VJoJB5fFleTz6xnZyMxP41a1TOKNfUmcW0Zheqcsax0VkpohsE5E8EXmwhflxIvKyN3+FiAwNmfeQN32biFwRMv0ZESkSkY2RLLvpHCLS5imoqCjhizNG8oe7zqGsOsDsx9/hlTUFnVhCY0xzEQsOEYkGHgdmAWOBW0RkbLPF7gIOq+oI4FHgJ966Y4G5wDhgJvArb3sAv/OmmV7k/BGZLPnSp5iYk8pX/7iOB19Zb8PhGtNFIlnjmAbkqeouVa0D5gOzmy0zG3jOe70QmCHu5+dsYL6q1qrqbiDP2x6quhwojWC5TTfVN9nPC3efw32fHs78Vflc/6v3eDfvEA3B0/90qzHdSSRbGgcC+SHvC4BzWltGVQMiUgZkeNM/aLbuwI58uIjcA9wDMHjw4A4V3HRfvugovn7FaKYOTeerC9Zx629X0DcpjmsmDmD2pAGcOTDFrr4yJsIiGRwt/e9t/tOwtWXCWbdNqvok8CS4xvGOrGu6v0+f0Zd3v3kJb24t4tW1+3j+/b08/c5ucjMTuHbiAK6dNIDhWYldXUxjTkuRDI4CYFDI+xxgfyvLFIiID0jBnYYKZ13Ty8XHRnPVhP5cNaE/ZUfreW3TAV5du59fvLmDny/dwajsRKYMSfO6h09jRN9EG8nQmFMgksGxChgpIrnAPlxj9781W2YxcAfwPjAHeFNVVUQWAy+KyM+AAcBIYGUEy2p6uJQ+Mdx89mBuPnswheU1/GXdfpbvOMTf1h/gpZXujGlinI8zB6YwaXAqZw1O48JRmdaXljEnIGLB4bVZ3A+8DkQDz6jqJhF5GFitqouBp4HnRSQPV9OY6627SUQWAJuBAHCfqjYAiMhLwMVApogUAN9T1acj9T1Mz5Od7OfuC4Zx9wXDCAaV3SVVrP34CGvz3eOp5bsIBJWMhFhumTaY284dQr8Uf1cX25gew24ANL1OTX0DK3aX8vz7e1m6tZBoEa4Y3487zx/KlCFp1rhuDDZ0rDHH8cdEc9GoLC4alcXHJUd5/oM9zF+Vz9/WH2DcgGTuOH8oF43KIjpKiBYhSoSoKIiOcq9joqOsrcT0albjMAY4Whdg0Uf7+N27e9hRVNnmsr4oYeKgVM4fnsF5wzI4a0ga/hhrKzGnF+uryoLDhElV+WBXKTuLXeeLwaDSoG56Q1BpUKWsup6Vu0tZX1BGQ1CJ9UVx1uBUzh+eyXnDM5g0KJWYaBvqxvRsFhwWHCYCKmrqWbWnlPd3lvDezhI2HyhHFVLiY7h8bDZXntmf6SMyifVZiJiex4LDgsN0giNH63h/Zwn/2FzIG5sLqagNkOT3cdmYbGaO78eFo7LslJbpMSw4LDhMJ6sNNPBeXglLNhzgH5sLKauuJyE2mvOGZ5CV5Cc9IYa0PrGk9YklPSGW1D4xpCfEkp3s77XhUlZdz4aCMtYVHGFvSRXn5GZw6dhsUuJtBMiuYMFhwWG6UH1DkPd3lvD3jQdZs7eU0qp6Dh+ta7VzxszEOHLS4hmYFk9OqnsemBpPbmYCQzMSiArjiq66QJCVu0tZurWQZVuLKKmqI9kfQ3J8DMl+n/ccQ3K8j2S/C620hFjS+sQ0hVl6QmzEQqyqNsDWg+Wsyy9jfcER1heUsetQVdP8JL+PipoAsdFRXDAykyvP7M9l47LbHXK4oyprA7y/s4Tl24t5e3sx5TX1XHVmf+ZMyWHSoNRTcml2dV0DWw+WMyo7KeyByFSVLQcqeH3TQXYUVTAqO4nxA1IYPzCF7OS4Trlk3ILDgsN0M6pKeU2AI0frKK2q4/BRN1zugbIa9h2uZt+RY4/QIXcT43yMHZDMmQNTOHNgCuMHJpOb6bpSOVxVx7JtRSzdUsTy7cVU1AaI80UxfUQmg9P7UF5TT3l1wHuup6LGva6oCbRazviYaLKS4uiX7Kdvsnvul+InO9k90hNiiY2Owhct7hHlXsdERSEC+45Us7u4it2Hqth1qIrdhyrZfaiKwvLaps/omxTHxEGpTMxJYUJOKhNyUkiJj+Gj/CMsWX+AJRsOsL+spilErprQn0mDUkmJjyHJH9OhNqTGA/Lb24tZvr2Y1XtLqW9Q+sRGc96wDPrE+fjn5oPU1AcZlpXAnCk5fGZyTodvEK2qDbBsWxF/33CQN7cWUV3fgC9KmJCT0nQRxZRmV+M1BJUPPz7M6xsP8vrmg+SXViMCA1Pj2XekmsZDdWZiLOMGuL/92P4p9ImLpi4QpL4hSF0g2PS6NhAkzhfFZ88b2qGyN7LgsOAwPVQwqByqqqXgcDV5RZVs3FfGhn1lbN5fTq0XKAmx0eSk9WFHUQVBdQfiGWP6MmN0NtNHZBIf23atIdAQpKza1YJKq+qbgqy0yj2KK2o5WF5DYXkNB8tqmj63o9L6xJCbmUBuZiLDshIY0TeRiTmp7R6Ug0FlbcHxIRIqPia6qeaUHB9Dn9hoagPuwFlb30Bd4+tAA0fr3ANgdL8kLjoji4tGZjFlaFpT9zMVNfUs2XCAhWsKWLXnMCLwqRGZfOasgQzLTCTR7yPJ7yMpLgZ/TFTTr/+Kmnre3FrEkg0HeGtbMbWBIJmJccwcn805uRlsOVDO+7tKjl2NFx3F5MGpnDssg8LyGt7YUsihyjpio6M4f0QGV4zrx6VjsslKimuqoW3cV87GfWVs3F/OjsIKAu0MKZCZGMvqb192Qn8vCw4LDnOaCTQEySuuZENBGRv3lbG75CiTBqVy6Zi+jB+QEtbprBOhqpRXBzhYXsPB8hqOHK2jvkEJNASpD7rnQIMSCCpBVfol+8nNSiA3I4G0hNiT/vxgUFlXcIQ9JVWu9lRdf3xNqqaeo3UNxPmiiPNFu+cY79mbNqZ/EheOyiI7uf1axJ5DVfzpwwJe+XAf+45Uf2K+L0qagqSwrJa6hiDZyXHMGt+fWeP7MXVo+iduFq2oqWf1nsO8t/MQ7+8qYdP+cvrERHPx6L5cMa4fnz4ji6QwTsnV1DeQV1RJfUOQWF8UsdFRxPqiiAl9jo5q94dDayw4LDiMMSchGFQ27CvjUGUtlbUBymsCVNYEqKipp7I2QEVNgMzEWGaO78fkQWkdCu6KmnpivVDrTqzLEWOMOQlRXm8BkRBO7aK7sTuTjDHGdIgFhzHGmA6x4DDGGNMhFhzGGGM6xILDGGNMh1hwGGOM6RALDmOMMR1iwWGMMaZDesWd4yJSDOw9wdUzgUOnsDinK9tP4bH9FB7bT+GL1L4aoqpZLc3oFcFxMkRkdWu33ZtjbD+Fx/ZTeGw/ha8r9pWdqjLGGNMhFhzGGGM6xIKjfU92dQF6CNtP4bH9FB7bT+Hr9H1lbRzGGGM6xGocxhhjOsSCwxhjTIdYcLRCRGaKyDYRyRORB7u6PN2JiDwjIkUisjFkWrqI/FNEdnjPaV1Zxu5ARAaJyDIR2SIim0TkAW+67asQIuIXkZUiss7bTz/wpueKyApvP70sIic/9uxpQESiReQjEfmr977T95MFRwtEJBp4HJgFjAVuEZGxXVuqbuV3wMxm0x4ElqrqSGCp9763CwBfVdUxwLnAfd6/I9tXx6sFLlHVicAkYKaInAv8BHjU20+Hgbu6sIzdyQPAlpD3nb6fLDhaNg3IU9VdqloHzAdmd3GZug1VXQ6UNps8G3jOe/0ccF2nFqobUtUDqvqh97oC9599ILavjqNOpfc2xnsocAmw0Jve6/cTgIjkAFcBv/XeC12wnyw4WjYQyA95X+BNM63LVtUD4A6YQN8uLk+3IiJDgcnACmxffYJ3+mUtUAT8E9gJHFHVgLeI/R90HgO+AQS99xl0wX6y4GiZtDDNrls2J0REEoFXgC+ranlXl6c7UtUGVZ0E5OBq/GNaWqxzS9W9iMjVQJGqrgmd3MKiEd9Pvkh/QA9VAAwKeZ8D7O+isvQUhSLSX1UPiEh/3C/HXk9EYnCh8YKq/smbbPuqFap6RETewrUJpYqIz/s1bf8HYTpwrYhcCfiBZFwNpNP3k9U4WrYKGOldrRALzAUWd3GZurvFwB3e6zuAV7uwLN2Cd/75aWCLqv4sZJbtqxAikiUiqd7reOBSXHvQMmCOt1iv30+q+pCq5qjqUNwx6U1VvZUu2E9253grvFR/DIgGnlHVR7q4SN2GiLwEXIzrzrkQ+B7wZ2ABMBj4GLhRVZs3oPcqIvIp4F/ABo6dk/5PXDuH7SuPiEzANepG437MLlDVh0VkGO7ClHTgI+A2Va3tupJ2HyJyMfA1Vb26K/aTBYcxxpgOsVNVxhhjOsSCwxhjTIdYcBhjjOkQCw5jjDEdYsFhjDGmQyw4jOnGROTixl5QjekuLDiMMcZ0iAWHMaeAiNzmjSmxVkR+43XaVyki/09EPhSRpSKS5S07SUQ+EJH1IrKocTwOERkhIm9441J8KCLDvc0nishCEdkqIi94d6Qb02UsOIw5SSIyBrgZmO511NcA3AokAB+q6lnA27g77AF+D3xTVSfg7ipvnP4C8Lg3LsX5wAFv+mTgy7ixYYbh+iwypstYJ4fGnLwZwBRglVcZiMd1XBgEXvaW+QPwJxFJAVJV9W1v+nPAH0UkCRioqosAVLUGwNveSlUt8N6vBYYC70T+axnTMgsOY06eAM+p6kPHTRT5TrPl2urfp63TT6H9DjVg/29NF7NTVcacvKXAHBHpC01jig/B/f9q7LX034B3VLUMOCwiF3jTPwu87Y3TUSAi13nbiBORPp36LYwJk/1yMeYkqepmEfk28A8RiQLqgfuAKmCciKwBynDtIOC6vv61Fwy7gDu96Z8FfiMiD3vbuLETv4YxYbPecY2JEBGpVNXEri6HMaeanaoyxhjTIVbjMMYY0yFW4zDGGNMhFhzGGGM6xILDGGNMh1hwGGOM6RALDmOMMR3y/wFQj1GfxC6vMQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy(epoch_train_acc, epoch_val_acc) \n",
    "plot_loss(epoch_train_loss, epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:478.17%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mixnet = timm.create_model('mixnet_xl', pretrained=True)\n",
    "mixnet.classifier = nn.Linear(1536, 11)\n",
    "mixnet.load_state_dict(torch.load(\"model_mixnet_val2_restart2.pkl\"))\n",
    "mixnet.cuda()\n",
    "mixnet.eval()\n",
    "running_test_accuracy = 0\n",
    "for i, batch in enumerate(val2_dl):\n",
    "    with torch.no_grad():\n",
    "        x = batch[0]\n",
    "        labels = batch[1]\n",
    "        x = x.cuda()\n",
    "        x = x.cuda()\n",
    "        labels = labels.cuda()\n",
    "        y = mixnet(x)\n",
    "        running_test_accuracy += (y.max(1)[1] == labels).sum().item()\n",
    "print(\"Test accuracy:{:.2f}%\".format(100* running_test_accuracy/float(len(val_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diretory of data\n",
    "batch_size = 32\n",
    "res_img_dir = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\food-11\\evaluation\"\n",
    "submission = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\\kaggle_evaluation\"\n",
    "res_img_dir2 = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\data\\training\"\n",
    "class testDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        # use glob to get all image names\n",
    "        self.img_names = [x.rsplit(\"\\\\\")[-1] for x in glob.glob(img_dir + \"\\*\") ]\n",
    "        \n",
    "        # PyTorch transforms\n",
    "        self.transform = transforms.Compose([#transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                             #transforms.RandomRotation(180),\n",
    "                                             #transforms.ColorJitter(),\n",
    "                                             # transforms.RandomRotation(45),\n",
    "                                             # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "                                                 transforms.Resize((256, 256)),\n",
    "                                             transforms.CenterCrop((224, 224)),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                            ])\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._read_img(i)\n",
    "\n",
    "    def _read_img(self, i):\n",
    "        img = Image.open(self.img_dir + \"\\\\\" + self.img_names[i])\n",
    "        return self.transform(img), self.img_names[i]\n",
    "res_test_dataset = testDataset(submission)\n",
    "test_dl = torch.utils.data.DataLoader(res_test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"benchmark_model_resNext102.pkl\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "net.eval()\n",
    "\n",
    "res_Id = []\n",
    "res_label = []\n",
    "\n",
    "for i, batch in enumerate(test_dl):\n",
    "    with torch.no_grad():\n",
    "        # Get a batch from the dataloader\n",
    "        x = batch[0]\n",
    "        labels = batch[1]\n",
    "        x = x.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Compute the network output\n",
    "        y = net(x)\n",
    "        \n",
    "        res_Id.append(x)\n",
    "        res_label.append(y)\n",
    "\n",
    "df = pd.DataFrame\n",
    "df[\"Id\"] = pd.Series(res_Id)\n",
    "df[\"Category\"] = pd.Series(res_label)\n",
    "df.to_csv(\"submission_pandas.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "#batch_size=25\n",
    "#res_test_dataset = testDataset(res_img_dir)\n",
    "#test_dl = torch.utils.data.DataLoader(res_test_dataset, batch_size=batch_size)\n",
    "#inception = models.inception_v3(pretrained=False,aux_logits=False)\n",
    "#inception.fc = nn.Linear(2048, 11)\n",
    "pre_state_dict = torch.load(\"benchmark_model_densenet_params (1).pkl\", map_location=\"cpu\")\n",
    "\n",
    "\n",
    "#inception = models.resnext101_32x8d(pretrained=False)\n",
    "#fc_features = inception.fc.in_features\n",
    "#inception.fc = nn.Linear(fc_features, 11)\n",
    "#inception.load_state_dict(pre_state_dict)\n",
    "#inception.eval()\n",
    "#inception.cuda()\n",
    "\n",
    "mixnet = timm.create_model('mixnet_xl', pretrained=False)\n",
    "mixnet.classifier = nn.Linear(1536, 11)\n",
    "mixnet.load_state_dict(torch.load(\"model_mixnet_val2.pkl\"))\n",
    "mixnet.cuda()\n",
    "mixnet.eval()\n",
    "\n",
    "with open('sample_submission.csv', \"w\") as f:\n",
    "    f.write(\"Id,Category\\n\")\n",
    "    for i, batch in enumerate(test_dl):\n",
    "        with torch.no_grad():\n",
    "            #x = torch.cat([res_test_dataset[i][0].unsqueeze(0) for _ in range(20)], 0)\n",
    "            x = batch[0]\n",
    "            filename = batch[1]\n",
    "            x = x.cuda()\n",
    "            y = mixnet(x)\n",
    "            pred = y.max(1)[1]\n",
    "            print(len(pred))\n",
    "            for j in range(len(pred)):\n",
    "                f.write(\"{}, {:d}\\n\".format((filename[j].split(\".\"))[0], (pred[j])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "#batch_size=25\n",
    "#res_test_dataset = testDataset(res_img_dir)\n",
    "#test_dl = torch.utils.data.DataLoader(res_test_dataset, batch_size=batch_size)\n",
    "#inception = models.inception_v3(pretrained=False,aux_logits=False)\n",
    "#inception.fc = nn.Linear(2048, 11)\n",
    "pre_state_dict = torch.load(\"benchmark_model_densenet_params (1).pkl\", map_location=\"cpu\")\n",
    "\n",
    "\n",
    "#inception = models.resnext101_32x8d(pretrained=False)\n",
    "#fc_features = inception.fc.in_features\n",
    "#inception.fc = nn.Linear(fc_features, 11)\n",
    "#inception.load_state_dict(pre_state_dict)\n",
    "#inception.eval()\n",
    "#inception.cuda()\n",
    "\n",
    "mixnet = timm.create_model('mixnet_xl', pretrained=False)\n",
    "mixnet.classifier = nn.Linear(1536, 11)\n",
    "mixnet.load_state_dict(torch.load(\"model_mixnet_val2.pkl\"))\n",
    "mixnet.cuda()\n",
    "mixnet.eval()\n",
    "\n",
    "with open('special2_sample_submission.csv', \"w\") as f:\n",
    "    f.write(\"Id,Category\\n\")\n",
    "    for i, batch in enumerate(test_dl):\n",
    "        with torch.no_grad():\n",
    "            #x = torch.cat([res_test_dataset[i][0].unsqueeze(0) for _ in range(20)], 0)\n",
    "            x = batch[0]\n",
    "            filename = batch[1]\n",
    "            x = x.cuda()\n",
    "            y = mixnet(x)\n",
    "            pred = y.max(1)[1]\n",
    "            print(len(pred))\n",
    "            for j in range(len(pred)):\n",
    "                if str(pred[j].tolist()) != str(filename[j].split(\".\")[0].split(\"_\")[0]):\n",
    "                    f.write(\"{}, {:d}, {}\\n\".format((filename[j].split(\".\"))[0], (pred[j]),(filename[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'resnext101_32x8d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-736c2ca517d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtest_dl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_test_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mresnet101\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnext101_32x8d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mset_parameter_requires_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet101\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'resnext101_32x8d'"
     ]
    }
   ],
   "source": [
    "#ensemble voting , resnet101 , mixnet, efficienNet , inception\n",
    "# each one has been trained before \n",
    "#For resnet\n",
    "import timm\n",
    "from collections import Counter\n",
    "\n",
    "res_img_dir = r\"C:\\Users\\THOMA\\Documents\\Computer Science and Math\\EIT UCA\\Deep Learning\\kaggle\\images\\food-11\\evaluation\"\n",
    "res_test_dataset = testDataset(res_img_dir)\n",
    "test_dl = torch.utils.data.DataLoader(res_test_dataset, batch_size=batch_size)\n",
    "\n",
    "resnet101 = models.resnext101_32x8d(pretrained=False)\n",
    "\n",
    "set_parameter_requires_grad(resnet101, True)\n",
    "fc_features = resnet101.fc.in_features\n",
    "resnet101.fc = nn.Linear(fc_features, 11)\n",
    "resnet101.load_state_dict(torch.load(\"benchmark_model_densenet_params (1).pkl\"))\n",
    "resnet101.cuda()\n",
    "resnet101.eval()\n",
    "#for Mixnet \n",
    "mixnet = timm.create_model('mixnet_xl', pretrained=True)\n",
    "mixnet.classifier = nn.Linear(1536, 11)\n",
    "mixnet.load_state_dict(torch.load(\"model_mixnet.pkl\"))\n",
    "mixnet.cuda()\n",
    "mixnet.eval()\n",
    "\n",
    "#for inception\n",
    "inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "inception.fc = nn.Linear(2048, 11)\n",
    "inception.load_state_dict(torch.load(\"inceptionV3_epoch_16_model.pth\"))\n",
    "inception.cuda()\n",
    "inception.eval()\n",
    "#for efficientNet\n",
    "#model = EfficientNet.from_pretrained(model_name='efficientnet-b5', num_classes=11)\n",
    "#mixnet.classifier = nn.Linear(1536, 11)\n",
    "#mixnet.load_state_dict(torch.load(\"mixnet.pkl\"))\n",
    "#mixnet.cuda()\n",
    "# Id  Category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [inception,mixnet,resnet101]\n",
    "with open('voting_model_submission.csv', \"w\") as f:\n",
    "    f.write(\"Id,Category\\n\")\n",
    "    for i, batch in enumerate(test_dl):\n",
    "        with torch.no_grad():\n",
    "            x = batch[0]\n",
    "            filename = batch[1]\n",
    "            x = x.cuda()\n",
    "            y_models = [model(x) for model in models]\n",
    "            pred_models = [y.max(1)[1].tolist() for y in y_models]\n",
    "            default = 1\n",
    "            #print(pred_models)\n",
    "            for j in range(len(pred_models[1])):\n",
    "                res = {i:pred_models[i][j] for i in range(3)}\n",
    "                tmp = list(res.values())\n",
    "                tmp = [e for e in tmp]\n",
    "                if len(set(tmp)) == len(models):\n",
    "                    print(set(tmp))\n",
    "                    f.write(\"{}, {:d}\\n\".format((filename[j].split(\".\"))[0], (res[default][j])))\n",
    "                else:\n",
    "                    vote = Counter()\n",
    "                    for key,items in res.items():\n",
    "                        vote[items] += 1\n",
    "                    vote_res = vote.most_common(1)\n",
    "                    f.write(\"{}, {:d}\\n\".format((filename[j].split(\".\"))[0], (vote_res[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "Ids = df[\"Id\"].values\n",
    "Cat = df[\"Category\"].values\n",
    "\n",
    "Ids = [int(e.split(\"_\")[0]) for e in Ids]\n",
    "Cat = [int(e) for e in Cat]\n",
    "count = 0 \n",
    "for i in range(len(Ids)):\n",
    "    if Cat[i] != Ids[i]:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeaturesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,f1,f2,f3,labellst):\n",
    "        self.feature1 = f1\n",
    "        self.feature2 = f2\n",
    "        self.feature3 = f3\n",
    "        self.labellst = labellst\n",
    "    \n",
    "    def __getitem__(self,index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
