{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0  Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!ls\n",
        "!pwd\n",
        "!ls /\n",
        "!ls /kaggle/\n",
        "!ls /kaggle/input/\n",
        "!ls /kaggle/input/polytech-ds-2019/\n",
        "!ls /kaggle/input/models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from IPython.display import FileLink\n",
        "os.chdir(r'/kaggle/working')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1  Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1  Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import glob\n",
        "from IPython.display import display\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm_notebook\n",
        "import random\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2  Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The original training and validation datasets contains 9866 and 3430 images respectively. The number of validation images are rather large, so we merged them together and re-split randomly. We used a split ratio of 10%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# diretory of data\n",
        "img_dir = [r\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/training/\", \n",
        "           r\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/validation/\"]\n",
        "\n",
        "class trainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, train_list):\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.train_list = train_list\n",
        "        self.img_names = [self.root_dir + os.sep + item for item in self.train_list]\n",
        "        self.labels = [int(item.split(os.sep)[1].split('_')[0]) for item in self.train_list]\n",
        "\n",
        "        self.transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                             #transforms.RandomRotation(180),\n",
        "                                             transforms.ColorJitter(brightness=(0.3 if random.random()<0.5 else False),\n",
        "                                                                    contrast=(0.2 if random.random()<0.5 else False),\n",
        "                                                                    saturation=(0.2 if random.random()<0.5 else False),\n",
        "                                                                    hue=(0.1 if random.random()<0.5 else False)),\n",
        "                                             transforms.RandomAffine(10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=(0.15, 0), resample=False),\n",
        "                                             transforms.Resize((256, 256)),\n",
        "                                             transforms.CenterCrop((224, 224)),\n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                                   [0.229, 0.224, 0.225])\n",
        "                                            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.img_names[i]).convert('RGB')\n",
        "        return self.transform(img), self.labels[i]\n",
        "\n",
        "\n",
        "\n",
        "class valDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, val_list):\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.val_list = val_list\n",
        "        self.img_names = [self.root_dir + os.sep + item for item in self.val_list]\n",
        "        self.labels = [int(item.split(os.sep)[1].split('_')[0]) for item in self.val_list]\n",
        "\n",
        "        # PyTorch transforms\n",
        "        self.transform = transforms.Compose([transforms.Resize((256, 256)),\n",
        "                                             transforms.CenterCrop((224, 224)),\n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                                  [0.229, 0.224, 0.225])\n",
        "                                            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.img_names[i]).convert('RGB')\n",
        "        return self.transform(img), self.labels[i]\n",
        "\n",
        "\n",
        "def random_split_train_val(train_dir, val_dir, split_ratio):\n",
        "    train_names = ['training' + os.sep + item for item in os.listdir(train_dir)]\n",
        "    val_names = ['validation' + os.sep + item for item in os.listdir(val_dir)]\n",
        "    all_names = train_names + val_names\n",
        "    nums = len(all_names)\n",
        "    ratio = split_ratio\n",
        "    random.shuffle(all_names)\n",
        "    new_val = all_names[:round(nums * ratio)]\n",
        "    new_train = all_names[round(nums * ratio):]\n",
        "    return new_train, new_val\n",
        "\n",
        "\n",
        "train_dir = r\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/training/\"\n",
        "val_dir = r\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/validation/\"\n",
        "new_train, new_val = random_split_train_val(train_dir, val_dir, 0.1)\n",
        "train_set = trainDataset(r\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/\", new_train)\n",
        "print(\"training dataset size\",len(train_set))\n",
        "val_set = valDataset(r\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/\", new_val)\n",
        "print(\"validation dataset size\",len(val_set))\n",
        "\n",
        "def display_tensor(t):\n",
        "    trans = transforms.ToPILImage()\n",
        "    display(trans(t))\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    img_train, label_train = train_set[i]\n",
        "    img_val, label_val = val_set[i]\n",
        "    display_tensor(img_train)\n",
        "    print(\"class : \", label_train)\n",
        "    display_tensor(img_val)\n",
        "    print(\"class : \", label_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Create Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We trained different models including InceptionV3, ResNext50, ResNext101, Mixnet_l, Mixnet_xl and EfficientNet_B5. We used the pretrained models on ImageNet and finetuned them on the food11 dataset. \n",
        "\n",
        "The first three models are loaded from Pytorch model zoo, among them Resnext101 has the best accuracy with the last 6 layers unfrozen (we didn't do elaborate experiments of this and this is the best we got). \n",
        "\n",
        "For Mixnet (https://arxiv.org/abs/1907.09595) we unfroze all the layers.\n",
        "\n",
        "For EfficientNet (https://arxiv.org/abs/1905.11946) we unfroze all the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 25\n",
        "test_split = 0.1\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# unfreezing function for certain layers, only used on ResNext        \n",
        "def unfreeze_last_layers(model, feature_extracting):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    next(model.layer4[1].conv1.parameters()).requires_grad = True\n",
        "    next(model.layer4[1].conv2.parameters()).requires_grad = True\n",
        "    next(model.layer4[1].conv3.parameters()).requires_grad = True\n",
        "    next(model.layer4[2].conv1.parameters()).requires_grad = True\n",
        "    next(model.layer4[2].conv2.parameters()).requires_grad = True\n",
        "    next(model.layer4[2].conv3.parameters()).requires_grad = True\n",
        "    next(model.layer4[0].conv1.parameters()).requires_grad = True\n",
        "    next(model.layer4[0].conv2.parameters()).requires_grad = True\n",
        "    next(model.layer4[0].conv3.parameters()).requires_grad = True\n",
        "    next(model.layer3[22].conv1.parameters()).requires_grad = True\n",
        "    next(model.layer3[22].conv2.parameters()).requires_grad = True\n",
        "    next(model.layer3[22].conv3.parameters()).requires_grad = True\n",
        "\n",
        "# model1 mixnet_l\n",
        "model = timm.create_model('mixnet_l', pretrained=True)\n",
        "model.classifier = nn.Linear(1536, 11)\n",
        "print(model)\n",
        "\n",
        "# # model2 resnext101\n",
        "# model = models.resnext101_32x8d(pretrained=True)\n",
        "# # unfreezing last layers in resnext in order to use the bottom features and reduce calculation (resnext101 model is quite big), \n",
        "# # which also yielded a better accuracy in our experiments\n",
        "# unfreeze_last_layers(model)\n",
        "# fc_features = model.fc.in_features\n",
        "# model.fc = nn.Linear(fc_features, 11)\n",
        "# print(model)\n",
        "\n",
        "# # model3 mixnet_xl\n",
        "# model = timm.create_model('mixnet_xl', pretrained=True)\n",
        "# model.classifier = nn.Linear(1536, 11)\n",
        "# print(model)\n",
        "\n",
        "# # model4 inceptionV3\n",
        "# model = torchvision.models.inception_v3(pretrained=True, aux_logits=False, transform_input=False)\n",
        "# model.fc = nn.Linear(2048, 11)\n",
        "# model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4  Training and Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our training process contains two phases: \n",
        "\n",
        "1. Using the CosineAnnealing scheduler. As the learning rates for each epoch are calculated automatically and sometimes the minimizing is interupted by the increasing learning rate in the ascending period of the cosine function, we added the second phase of training, tuned the learning rate in a more controlable way. \n",
        "\n",
        "2. Using the MultiStep scheduler. From the optimal model obtained in phase 1, we restart the training with a smaller learning rate and obtained our final optimal model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.1  Phase 1 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "LEARNING_RATE = 0.01\n",
        "model.cuda()\n",
        "N_EPOCHS = 50\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE, momentum=0.9, weight_decay=1e-4) \n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "epoch_val_loss = [] \n",
        "epoch_val_acc = []\n",
        "epoch_train_loss = []\n",
        "epoch_train_acc = []\n",
        "flag = 0\n",
        "best_val_acc = 0\n",
        "for e in range(N_EPOCHS):\n",
        "    print(\"EPOCH:\",e)\n",
        "    running_loss = 0\n",
        "    running_accuracy = 0\n",
        "    model.train()\n",
        "    for i, batch in enumerate(tqdm_notebook(train_dl)):\n",
        "#         # quick check for max batch size\n",
        "        if i == 5:\n",
        "            break\n",
        "        x = batch[0]\n",
        "        labels = batch[1]\n",
        "        x = x.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        y = model(x)\n",
        "        loss = criterion(y, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_accuracy += (y.max(1)[1] == labels).sum().item()\n",
        "    \n",
        "    print(\"Training accuracy: {:.2f}%\".format(100*running_accuracy/float(len(train_set))),\n",
        "          \"Training loss:\", running_loss/float(len(train_dl)), \"learning rate:\", scheduler.get_lr()[0])\n",
        "    \n",
        "    scheduler.step()\n",
        "    epoch_train_acc.append(running_accuracy/float(len(train_set)))\n",
        "    epoch_train_loss.append(running_loss/float(len(train_set)))\n",
        "    \n",
        "\n",
        "    model.eval()\n",
        "    running_val_loss = 0\n",
        "    running_val_accuracy = 0\n",
        "    \n",
        "    for i, batch in enumerate(val_dl):\n",
        "        with torch.no_grad():\n",
        "#             # quick check for max batch size\n",
        "            if i == 5:\n",
        "                break\n",
        "            x = batch[0]\n",
        "            labels = batch[1]\n",
        "            x = x.cuda()\n",
        "            labels = labels.cuda()\n",
        "            y = model(x)\n",
        "            loss = criterion(y, labels)\n",
        "            running_val_loss += loss.item()\n",
        "            running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n",
        "    acc = running_val_accuracy/float(len(val_set))\n",
        "    if acc > best_val_acc:\n",
        "        best_val_acc = acc\n",
        "        torch.save(model.state_dict(), '/kaggle/working/model_mixnet_phase1.pkl')\n",
        "        flag = 0\n",
        "    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n",
        "          \"Validation loss:\", running_val_loss/float(len(val_set)))\n",
        "    epoch_val_loss.append(running_val_loss/len(val_set))\n",
        "    epoch_val_acc.append(running_val_accuracy/len(val_set))\n",
        "\n",
        "\n",
        "# model=mixnet_xl, scheduler=CosineAnnealing, lr=0.01\n",
        "# Training accuracy: 99.87% Training loss: 0.0056521773303466446 learning rate: 0.006545084971874732\n",
        "# Validation accuracy:96.71% Validation loss: 0.0057580280123060035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization of Loss and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_accuracy(train_acc, val_acc):\n",
        "    plt.title('FOOD11')\n",
        "    plt.plot(train_acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n",
        "    plt.show()\n",
        "    \n",
        "def plot_loss(train_loss, val_loss):\n",
        "    plt.title('FOOD11')\n",
        "    plt.plot(train_loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_loss', 'validation_loss'], loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plot_accuracy(epoch_train_acc, epoch_val_acc) \n",
        "plot_loss(epoch_train_loss, epoch_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.2  Phase 2 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We restarted training with a different scheduler and smaller learning rate, with the best model obatined by CosineAnnealing (phase1), did fine-grained learning rate control using multistep scheduler and obtained the final best model (phase2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# restart_training = timm.create_model('mixnet_xl', pretrained=True)\n",
        "restart_training = timm.create_model('mixnet_l', pretrained=True)\n",
        "restart_training.classifier = nn.Linear(1536, 11)\n",
        "restart_training.load_state_dict(torch.load('/kaggle/working/model_mixnet_phase1.pkl'))\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "LEARNING_RATE = 0.001\n",
        "restart_training.cuda()\n",
        "\n",
        "N_EPOCHS = 50\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(restart_training.parameters(), lr = LEARNING_RATE, momentum=0.9, weight_decay=1e-4) \n",
        "MILESTONE = [20, 30, 40]\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONE, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "epoch_val_loss = [] \n",
        "epoch_val_acc = []\n",
        "epoch_train_loss = []\n",
        "epoch_train_acc = []\n",
        "best_val_acc = 0\n",
        "flag = 0\n",
        "for e in range(N_EPOCHS):\n",
        "    print(\"EPOCH:\",e)\n",
        "    running_loss = 0\n",
        "    running_accuracy = 0\n",
        "    restart_training.train()\n",
        "    for i, batch in enumerate(tqdm_notebook(train_dl)):\n",
        "#         # quick check\n",
        "        if i == 5:\n",
        "            break\n",
        "        x = batch[0]\n",
        "        labels = batch[1]\n",
        "        x = x.cuda()\n",
        "        labels = labels.cuda()\n",
        "        y = restart_training(x)\n",
        "        loss = criterion(y, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        running_accuracy += (y.max(1)[1] == labels).sum().item()\n",
        "    \n",
        "    print(\"Training accuracy: {:.2f}%\".format(100*running_accuracy/float(len(train_set))),\n",
        "          \"Training loss:\", running_loss/float(len(train_dl)), \"learning rate:\", scheduler.get_lr()[0])\n",
        "    \n",
        "    scheduler.step()\n",
        "    epoch_train_acc.append(running_accuracy/float(len(train_set)))\n",
        "    epoch_train_loss.append(running_loss/float(len(train_set)))\n",
        "    \n",
        "    restart_training.eval()\n",
        "\n",
        "    running_val_loss = 0\n",
        "    running_val_accuracy = 0\n",
        "    \n",
        "    for i, batch in enumerate(val_dl):\n",
        "        with torch.no_grad():\n",
        "#             # quick check for max batch size\n",
        "            if i == 5:\n",
        "                break\n",
        "            x = batch[0]\n",
        "            labels = batch[1]\n",
        "            x = x.cuda()\n",
        "            labels = labels.cuda()\n",
        "            y = restart_training(x)\n",
        "            loss = criterion(y, labels)\n",
        "            running_val_loss += loss.item()\n",
        "            running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n",
        "    acc = running_val_accuracy/float(len(val_set))\n",
        "    if acc > best_val_acc:\n",
        "        best_val_acc = acc\n",
        "        torch.save(restart_training.state_dict(), '/kaggle/working/model_mixnet_phase2.pkl')\n",
        "        flag = 0\n",
        "    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n",
        "          \"Validation loss:\", running_val_loss/float(len(val_set)))\n",
        "    epoch_val_loss.append(running_val_loss/len(val_set))\n",
        "    epoch_val_acc.append(running_val_accuracy/len(val_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "restart_training.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for i, batch in enumerate(val_dl):\n",
        "    with torch.no_grad():\n",
        "        x = batch[0]\n",
        "        labels = batch[1]\n",
        "        x = x.cuda()\n",
        "        labels = labels.cuda()\n",
        "        y = restart_training(x)        \n",
        "        y_true.extend(y.max(1)[1].tolist())\n",
        "        y_pred.extend(labels.tolist())\n",
        "print(\"confusion matrix\")\n",
        "confusion_matrix(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization of Loss and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plot_accuracy(epoch_train_acc, epoch_val_acc) \n",
        "plot_loss(epoch_train_loss, epoch_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2  Model Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "With the multiple trained models at hand, we did a simple ensembling of those models to see if there will be improvement.\n",
        "\n",
        "Our method of ensemble is averaging the predictions (output probabilities) of different models and obtain the final output prediction (averaging is simplified as calculating the sum of predictions in our code).\n",
        "\n",
        "The pre-trained models are uploaded as \"datasets\" in kaggle (which we have set them to be public) and added into this notebook. They should be seen under /kaggle/input/models/. EfficientNet doesn't work well both individually and ensemble, so we didn't include nor upload the model.\n",
        "\n",
        "Among all the ensemble combinations (tried by enumeration), the best accuracy is obtained by the ensemble of (resnext101, mixnet_l, mixnet_xl). \n",
        "\n",
        "Plus, we occasionally observed an improvement by zooming in the input image to a larger size when ensemble testing (training: 224, testing: 299), so we kept this method in the final submission. However for individual models this doesn't hold all the time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import os\n",
        "from PIL import Image\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "import timm\n",
        "import pandas as pd\n",
        "\n",
        "class food11_dataset_test(Dataset):\n",
        "\tdef __init__(self, root_dir, inp_list, img_transform=None):\n",
        "\t\tself.imgs = inp_list\n",
        "\t\tself.transform = img_transform\n",
        "\t\tself.img_dirs = [os.path.join(root_dir, img_dir) for img_dir in self.imgs]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.img_dirs)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\timg = Image.open(self.img_dirs[index]).convert('RGB')\n",
        "\t\tif self.transform is not None:\n",
        "\t\t\timg = self.transform(img)\n",
        "\t\tname = self.imgs[index].split('.')[0]\n",
        "\t\treturn [img, name]\n",
        "    \n",
        "def test_ensemble(test_dir, csv_dir):\n",
        "    transform_test = transforms.Compose([\n",
        "\t\ttransforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),\n",
        "\t\ttransforms.CenterCrop((CROP_SIZE, CROP_SIZE)),\n",
        "\t\ttransforms.ToTensor(),\n",
        "\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_list = os.listdir(test_dir)\n",
        "    test_dataset = food11_dataset_test(test_dir, test_list, transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n",
        "    # print(test_dataset[0])\n",
        "    N_batch_test = len(test_loader)\n",
        "\n",
        "    ckpt_1 = r'/kaggle/input/models/mixnet_l2_best.pkl'\n",
        "    ckpt_2 = r'/kaggle/input/models/resnext101_best.pkl'\n",
        "    ckpt_3 = r'/kaggle/input/models/model_mixnet_best.pkl'\n",
        "    ckpt_4 = r'/kaggle/input/models/inceptionV3_epoch_30_model.pth'\n",
        "\n",
        "    \n",
        "    # mixnet_l\n",
        "    model_1 = timm.create_model('mixnet_l', pretrained=False)\n",
        "    model_1.classifier = nn.Linear(1536, 11)\n",
        "    model_1.load_state_dict(torch.load(ckpt_1))\n",
        "    model_1.cuda()\n",
        "    model_1.eval()\n",
        "    \n",
        "    # resnext_100\n",
        "    model_2 = torchvision.models.resnext101_32x8d(pretrained=False)\n",
        "    fc_features = model_2.fc.in_features\n",
        "    model_2.fc = nn.Linear(fc_features, 11)\n",
        "    model_2.load_state_dict(torch.load(ckpt_2))\n",
        "    model_2.cuda()\n",
        "    model_2.eval()\n",
        "\n",
        "    # # mixnet_xl\n",
        "    model_3 = timm.create_model('mixnet_xl', pretrained=False)\n",
        "    model_3.classifier = nn.Linear(1536, 11)\n",
        "    model_3.load_state_dict(torch.load(ckpt_3))\n",
        "    model_3.cuda()\n",
        "    model_3.eval()\n",
        "\n",
        "    # inception_v3\n",
        "    # model_4 = torchvision.models.inception_v3(pretrained=False, aux_logits=False, transform_input=False)\n",
        "    # model_4.fc = nn.Linear(2048, 11)\n",
        "    # model_4.load_state_dict(torch.load(ckpt_4))\n",
        "    # model_4.cuda()\n",
        "    # model_4.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "     \n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    imgs = []\n",
        "    cate = []\n",
        "    start = time.time()\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        x = batch[0].cuda()\n",
        "        y1 = model_1(x)\n",
        "        y2 = model_2(x)\n",
        "        y3 = model_3(x)\n",
        "        # y4 = model_4(x)\n",
        "        y_fuse = y1 + y2 + y3\n",
        "\n",
        "    # for i, batch in enumerate(test_loader):\n",
        "    #     x = batch[0].cuda()\n",
        "    #     y2 = model_2(x)\n",
        "\n",
        "        names = batch[1]\n",
        "        for name in names:\n",
        "            imgs.append(name)\n",
        "        for pred in y_fuse.max(1)[1]:\n",
        "            cate.append(' '+str(pred.item()))\n",
        "        print(\"Processing batch %d/%d\" % (i, len(test_loader)))\n",
        "\n",
        "    end = time.time()\n",
        "    info = \"Test finished, elapsed_time=%.3f.\" % (end - start)\n",
        "    print(info)\n",
        "    df = pd.DataFrame()\n",
        "    df['Id'] = pd.Series(imgs)\n",
        "    df['Category'] = pd.Series(cate)\n",
        "    df.to_csv(csv_dir, index=False)\n",
        "    print(\"csv file wrote successfully.\")\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    RESIZE_SIZE = 342\n",
        "    CROP_SIZE = 299\n",
        "    BATCH_SIZE_VAL = 4\n",
        "\n",
        "    root_dir = \"/kaggle/input/polytech-ds-2019/polytech-ds-2019/\"\n",
        "    test_dir = root_dir + \"kaggle_evaluation/\"\n",
        "    csv_dir = \"result.csv\"\n",
        "    test_ensemble(test_dir, csv_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!rm /kaggle/working/result.csv\n",
        "!ls /kaggle/working/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3  Test-time Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We did experiments on test-time augmentation, using five-crop and ten-crop transformation, but found no improvements in accuracy. \n",
        "So in our final version we keep the centercrop testing transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import os\n",
        "from PIL import Image\n",
        "import time\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "import timm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class food11_dataset_centercrop(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, val_list, img_transform=None):\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.val_list = val_list\n",
        "        self.img_names = [self.root_dir + os.sep + item for item in self.val_list]\n",
        "        self.labels = [int(item.split(os.sep)[1].split('_')[0]) for item in self.val_list]\n",
        "        self.transform = img_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.img_names[i]).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, self.labels[i]\n",
        "    \n",
        "\n",
        "class food11_dataset_fivecrop(Dataset):\n",
        "\tdef __init__(self, root_dir, inp_list, img_transform=None):\n",
        "\t\tself.imgs = inp_list\n",
        "\t\tself.transform = img_transform\n",
        "\t\tself.img_dirs = [os.path.join(root_dir, img_dir) for img_dir in self.imgs]\n",
        "\t\tself.labels = [int(img_name.split('/')[1].split('_')[0]) for img_name in self.imgs]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.labels)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\timg = Image.open(self.img_dirs[index]).convert('RGB')\n",
        "\t\tif self.transform is not None:\n",
        "\t\t\timg = self.transform(img)\n",
        "\t\tlabel = self.labels[index]\n",
        "\t\treturn img, label\n",
        "\n",
        "    \n",
        "    \n",
        "def test_tencrop(test_dir, val_list, ckpt_dir):\n",
        "\ttransform_test = transforms.Compose([\n",
        "\t\ttransforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),\n",
        "\t\ttransforms.TenCrop((CROP_SIZE, CROP_SIZE)),\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\t\ttransforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(transforms.ToTensor()(crop)) for crop in crops]))\n",
        "        # transforms.ToTensor(),\n",
        "\t])\n",
        "\ttest_set = food11_dataset_fivecrop(test_dir, val_list, transform_test)\n",
        "\ttest_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n",
        "    # print(test_dataset[0])\n",
        "\tN_batch_test = len(test_loader)\n",
        "\n",
        "    # model = torchvision.models.inception_v3(pretrained=False, aux_logits=False, transform_input=False)\n",
        "    # model.fc = nn.Linear(2048, 11)\n",
        "    # state_dict = torch.load(ckpt_dir, map_location='cpu')\n",
        "    # model.load_state_dict(state_dict)\n",
        "\n",
        "\tmodel = timm.create_model('mixnet_xl', pretrained=False)\n",
        "\tmodel.classifier = nn.Linear(1536, 11)\n",
        "\tmodel.load_state_dict(torch.load(ckpt_dir))\n",
        "\tmodel.cuda()\n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\tmodel.eval()\n",
        "\trunning_loss = 0\n",
        "\trunning_acc = 0\n",
        "\tstart = time.time()\n",
        "\tfor i, batch in enumerate(test_loader):\n",
        "\t\tx = batch[0].cuda()\n",
        "\t\tlabels = batch[1].cuda()\n",
        "# \t\tprint(labels.size())\n",
        "        # print(x.size())\n",
        "\t\tbs, ncrops, c, h, w = x.size()\n",
        "\t\tresult = model(x.view(-1, c, h, w))\n",
        "\t\tresult_avg = result.view(bs, ncrops, -1).mean(1)\n",
        "\t\tloss = criterion(result_avg, labels)\n",
        "\t\trunning_loss += loss.item()\n",
        "\t\trunning_acc += (result_avg.max(1)[1] == labels).sum().item()\n",
        "\t\tprint(\"testing batch %d/%d\" % (i, len(test_loader)))\n",
        "\n",
        "\tend = time.time()\n",
        "\ttop1_acc = running_acc / len(test_dataset)\n",
        "\tinfo = \"test result: val_loss=%.3f, top1_acc=%.3f, elapsed_time=%.3f\" % (\n",
        "\t\trunning_loss / len(test_loader), top1_acc, end-start)\n",
        "\tprint(info)\n",
        "\n",
        "\n",
        "def test_fivecrop(test_dir, val_list, ckpt_dir):\n",
        "\ttransform_test = transforms.Compose([\n",
        "\t\ttransforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),\n",
        "\t\ttransforms.FiveCrop((CROP_SIZE, CROP_SIZE)),\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\t\ttransforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(transforms.ToTensor()(crop)) for crop in crops]))\n",
        "        # transforms.ToTensor(),\n",
        "\t])\n",
        "\n",
        "\ttest_list = os.listdir(test_dir)\n",
        "\ttest_dataset = food11_dataset_fivecrop(test_dir, val_list, transform_test)\n",
        "\ttest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n",
        "    # print(test_dataset[0])\n",
        "\tN_batch_test = len(test_loader)\n",
        "\n",
        "    # model = torchvision.models.inception_v3(pretrained=False, aux_logits=False, transform_input=False)\n",
        "    # model.fc = nn.Linear(2048, 11)\n",
        "    # state_dict = torch.load(ckpt_dir, map_location='cpu')\n",
        "    # model.load_state_dict(state_dict)\n",
        "\n",
        "\tmodel = timm.create_model('mixnet_xl', pretrained=False)\n",
        "\tmodel.classifier = nn.Linear(1536, 11)\n",
        "\tmodel.load_state_dict(torch.load(ckpt_dir))\n",
        "\tmodel.cuda()\n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "\tmodel.eval()\n",
        "\trunning_loss = 0\n",
        "\trunning_acc = 0\n",
        "\tstart = time.time()\n",
        "\tfor i, batch in enumerate(test_loader):\n",
        "\t\tx = batch[0].cuda()\n",
        "\t\tlabels = batch[1].cuda()\n",
        "# \t\tprint(labels.size())\n",
        "        # print(x.size())\n",
        "\t\tbs, ncrops, c, h, w = x.size()\n",
        "\t\tresult = model(x.view(-1, c, h, w))\n",
        "\t\tresult_avg = result.view(bs, ncrops, -1).mean(1)\n",
        "\t\tloss = criterion(result_avg, labels)\n",
        "\t\trunning_loss += loss.item()\n",
        "\t\trunning_acc += (result_avg.max(1)[1] == labels).sum().item()\n",
        "\t\tprint(\"testing batch %d/%d\" % (i, len(test_loader)))\n",
        "\n",
        "\tend = time.time()\n",
        "\ttop1_acc = running_acc / len(test_dataset)\n",
        "\tinfo = \"test result: val_loss=%.3f, top1_acc=%.3f, elapsed_time=%.3f\" % (\n",
        "\t\trunning_loss / len(test_loader), top1_acc, end-start)\n",
        "\tprint(info)\n",
        "\n",
        "    \n",
        "def test_centercrop(test_dir, val_list, ckpt_dir):\n",
        "\ttransform_test = transforms.Compose([\n",
        "\t\ttransforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),\n",
        "\t\ttransforms.CenterCrop((CROP_SIZE, CROP_SIZE)),\n",
        "\t\ttransforms.ToTensor(),\n",
        "\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\t])\n",
        "\n",
        "\ttest_dataset = food11_dataset_centercrop(root_dir, val_list, transform_test)\n",
        "\ttest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n",
        "    # print(test_dataset[0])\n",
        "\tN_batch_test = len(test_loader)\n",
        "\n",
        "    # model = torchvision.models.inception_v3(pretrained=False, aux_logits=False, transform_input=False)\n",
        "    # model.fc = nn.Linear(2048, 11)\n",
        "    # state_dict = torch.load(ckpt_dir, map_location='cpu')\n",
        "    # model.load_state_dict(state_dict)\n",
        "\n",
        "\tmodel = timm.create_model('mixnet_xl', pretrained=False)\n",
        "\tmodel.classifier = nn.Linear(1536, 11)\n",
        "\tmodel.load_state_dict(torch.load(ckpt_dir))\n",
        "\tmodel.cuda()\n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\tmodel.eval()\n",
        "\trunning_loss = 0\n",
        "\trunning_acc = 0\n",
        "\tstart = time.time()\n",
        "\tfor i, batch in enumerate(test_loader):\n",
        "\t\tx = batch[0].cuda()\n",
        "\t\tlabels = batch[1].cuda()\n",
        "# \t\tprint(labels.size())\n",
        "\t\ty = model(x)\n",
        "\t\tloss = criterion(y, labels)\n",
        "\t\trunning_loss += loss.item()\n",
        "\t\trunning_acc += (y.max(1)[1] == labels).sum().item()\n",
        "\t\tprint(\"testing batch %d/%d\" % (i, len(test_loader)))\n",
        "\n",
        "\tend = time.time()\n",
        "\ttop1_acc = running_acc / len(test_dataset)\n",
        "\tinfo = \"test result: val_loss=%.3f, top1_acc=%.3f, elapsed_time=%.3f\" % (\n",
        "\t\trunning_loss / len(test_loader), top1_acc, end-start)\n",
        "\tprint(info)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "RESIZE_SIZE = 256\n",
        "CROP_SIZE = 224\n",
        "RAND_PROB = 0.5\n",
        "BATCH_SIZE_VAL = 1\n",
        "\n",
        "root_dir = r\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/\"\n",
        "ckpt_dir = r\"/kaggle/input/models/model_mixnet_best.pkl\"\n",
        "# \ttest_centercrop(root_dir, new_val, ckpt_dir)\n",
        "# \ttest_fivecrop(root_dir, new_val, ckpt_dir)\n",
        "test_tencrop(root_dir, new_val, ckpt_dir)\n",
        "\n",
        "\n",
        "# cropsize=224\n",
        "# center crop test result: val_loss=0.132, top1_acc=0.970, elapsed_time=45.791\n",
        "# ten crop test result: val_loss=0.122, top1_acc=0.967, elapsed_time=380.180\n",
        "\n",
        "# cropsize=299\n",
        "# center no crop test result: val_loss=0.142, top1_acc=0.962, elapsed_time=33.212\n",
        "# center crop test result: val_loss=0.133, top1_acc=0.966, elapsed_time=35.501\n",
        "# five crop test result: val_loss=0.126, top1_acc=0.965, elapsed_time=449.507\n",
        "# ten crop test result: val_loss=0.123, top1_acc=0.966, elapsed_time=115.089"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "{}"
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "5 progress\n",
        "\n",
        "we have done 3 Big part \n",
        "\n",
        "1) The model part \n",
        "1.2) Image augmentation \n",
        "we merged the training and validation and re-splited randomly. We used a split ratio of 10%. we resize images to (299,299) then center cropped to (224,224) , \n",
        "we normalized the image , and added color jitter, randomflip , randomaffine as our choice for image augmentation\n",
        "1.3) creating model \n",
        "We trained different models InceptionV3, ResNext50, ResNext101, Mixnet_l, Mixnet_xl and EfficientNet_B5. We used the pretrained models on ImageNet and finetuned them on the food11 dataset.\n",
        "\n",
        "For Mixnet (https://arxiv.org/abs/1907.09595) we unfroze all the layers.\n",
        "\n",
        "For EfficientNet (https://arxiv.org/abs/1905.11946) we unfroze all the layers.\n",
        "\n",
        "1.4) Training \n",
        "Our training process contains two phases:\n",
        "\n",
        "Using the CosineAnnealing scheduler. As the learning rates for each epoch are calculated automatically and sometimes the minimizing is interupted by the increasing learning rate in the ascending period of the cosine function, we added the second phase of training, tuned the learning rate in a more controlable way.\n",
        "\n",
        "Using the MultiStep scheduler. From the optimal model obtained in phase 1, we restart the training with a smaller learning rate and obtained our final optimal model.\n",
        "1.4.2) Phase2 training\n",
        "We restarted training with a different scheduler and smaller learning rate, with the best model obatined by CosineAnnealing (phase1), did fine-grained learning rate control using multistep scheduler and obtained the final best model (phase2).\n",
        "\n",
        "2) Ensemble modeling \n",
        "With the multiple trained models at hand, we did a simple ensembling of those models to see if there will be improvement.\n",
        "\n",
        "Our method of ensemble is averaging the predictions (output probabilities) of different models and obtain the final output prediction (averaging is simplified as calculating the sum of predictions in our code).\n",
        "\n",
        "The pre-trained models are uploaded as \"datasets\" in kaggle (which we have set them to be public) and added into this notebook. They should be seen under /kaggle/input/models/. EfficientNet doesn't work well both individually and ensemble, so we didn't include nor upload the model.\n",
        "\n",
        "Among all the ensemble combinations (tried by enumeration), the best accuracy is obtained by the ensemble of (resnext101, mixnet_l, mixnet_xl). \n",
        "\n",
        "Plus, we occasionally observed an improvement by zooming in the input image to a larger size when ensemble testing (training: 224, testing: 299), so we kept this method in the final submission. However for individual models this doesn't hold all the time. \n",
        "\n",
        "\n",
        "3) Test Time augmentation\n",
        "\n",
        "We did experiments on test-time augmentation, using five-crop and ten-crop transformation, but found no improvements in accuracy. \n",
        "So in our final version we keep the centercrop testing transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Conclusion \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.7.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}